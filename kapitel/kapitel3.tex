% kapitel3.tex
\chapter{Algorithmus} \label{sec:alg}
Für das hier implementierte RAD-Sequencing-Tool, NodeRAD, wurde zur Workflowintegration das Workflow Management System Snakemake verwendet ~\cite{koester_2012_1, koester_2012_2}. Die einzelnen Analyseschritte werden dabei über Regeln abgebildet. Für jede Regel können neben dem zu verwendenden Script oder Shell-Kommando sowie den Pfadangaben für In- und Output auch zusätzliche Optionen festgelegt werden. Dazu gehören beispielsweise Angaben zu Parametern bzw. Argumenten für die verwendeten Tools, Pfadangaben für Log-Dateien oder die Anzahl der zu verwendenden Threads.

Als Input benötigt der Workflow eine Datei im FASTQ-Format, welche die single-end Reads der verschiedenen Individuen mit ihren Identifikationsbezeichnungen, der Basensequenz und Angaben zur  Basenqualität enthält. Des Weiteren wird eine Tabelle im tsv-Format benötigt, in der die Zuordnung der Probennamen zu den Individuen und ihren Barcode-Sequenzen definiert ist. Nach dem Preprocessing, der Qualitätskontrolle der Reads und dem Sequence-Alignment erfolgt die RAD-Seq-Analyse durch NodeRAD. Hierbei werden die Wahrscheinlichkeiten der Allelsequenzen und der möglichen Loci bestimmt. Die Loci mit der höchsten Wahrscheinlichkeit werden schließlich mit den Sequenzen ihrer Allele und den möglichen Varianten entsprechend dem ermittelten Genotyp in einer Datei im Variant Call Format (VCF) ausgegeben.

\section{Preprocessing} \label{sec:preproc}

Im Preprocessing werden durch das Tool Cutadapt ~\cite{martin_2011} die Reads jedes Individuums anhand ihrer Barcodesequenzen identifiziert und extrahiert (Demultiplexing). Hiernach werden die Barcodesequenzen entfernt (Trimming) und die Reads jedes Individuums in separaten Dateien im FASTQ Format abgelegt. \\
Im Anschluss an das Trimming erfolgt eine Qualitätskontrolle durch das Tool FastQC  ~\cite{andrews_2012}. Dabei werden einige allgemeine Statistiken zu den Rohdaten der Reads generiert, wie beispielsweise zur Basenqualität, zum GC-Gehalt, dem Anteil an Duplikaten oder überrepräsentierten Sequenzen. Durch das Tool MultiQC ~\cite{ewels_2016} wird aus diesen Statistiken und den Log-Dateien von Cutadapt ein html-Report mit diversen Plots zur Veranschaulichung erstellt.

\section{Edit-Distanzen} \label{sec:edit}
Für die spätere Konstruktion eines Graphen basierend auf den Edit-Distanzen zwischen den Readsequenzen wird für jedes Individuum zunächst ein Sequenzalignment mit Hilfe des Tools Minimap2 ~\cite{li_2018} erstellt. Hierbei werden alle Readsquenzen paarweise verglichen und in Abhängigkeit von ihren Übereinstimmungen (Matches) und Unterschieden (Mismatches) einander zugeordnet. Das Ergebnis des Mappings wird im sam-Format ~\cite{li_2009} ausgegeben und enthält Angaben zur betrachteten Sequenz (Query), die gegen einen anderen Read (Reference) verglichen wurde. Neben den ID's der Query- und Reference-Sequenzen, werden dort unter anderem auch der CIGAR-String, Informationen zur Basenqualität der Query-Sequenz, sowie optional verschiedene Tags angegeben. Ein für die späteren Berechnungen wichtiges Maß sind die Edit-Distanzen, die durch den NM-Tag repräsentiert werden. Die Edit-Distanz gibt hierbei die minimale Anzahl von Editieroperationen an, um die Query-Sequenz in die Referenzsequenz zu transformieren. Als Editieroperationen sind hierbei ersetzen, einfügen und löschen von Basen möglich. Auf DNA-Ebene entspricht dies den Punktmutationen im Sinne von Substitutionen, Insertionen und Deletionen (vgl. Kap. \ref{subsec:mutation}). Der CIGAR-String ist eine kondensierte Darstellung der Unterschiede zwischen Query- und Reference-Sequenz. In ihm werden Matches und Mismatches wie Insertionen, Substitutionen und Deletion jeweils mit der Anzahl der betroffenen Basen angegeben. Sowohl der CIGAR-String als auch der NM-Tag definieren wichtige Kanteneigenschaften des späteren Graphen. \\

\section{Konstruktion des Graphen} \label{sec:graph}
\subsection{Knoten des Graphen} \label{subsec:nodes}
Das hier in Python implementierte Tool, NodeRAD, benötigt als Input zu jedem Individuum die getrimmten single-end Read-Daten sowie das Sequenzalignment. Zunächst wird daraus für jedes Individuum ein eigener, gerichteter Graph $ G $ mit $ G = (V,E) $ erstellt. Seine Knoten, $ V $, werden durch die einzelnen Reads repräsentiert. Entsprechend ergeben sich die Knoteneigenschaften aus den Daten der Reads, diese werden den FASTQ-Dateien nach Ausführung von Cutadapt (siehe \ref{sec:preproc}) entnommen. Die Kanten, $ E $, zwischen den Knoten ergeben sich aus dem Vergleich ihrer Sequenzen im Rahmen des Sequenzalignments mittels Minimap2 (siehe \ref{sec:edit}).\\

Zusätzlich entnimmt NodeRAD der Konfigurationsdatei des Workflows einige Konstanten und Grenzwerte für die späteren Berechnungen. Dazu gehören die Mutationsraten und Heterozygotiewahrscheinlichkeiten für Substitutionen, Insertionen und Deletionen, die Ploidie des Chromosomensatzes der untersuchten Spezies und Grenzwerte. Die Konstanten werden als Grapheigenschaften im Graphen abgelegt. Als konfigurierbare Grenzwerte gibt es für NodeRAD einen Schwellenwert für die maximal zulässige Editierdistanz, bei dem zwei Knoten noch durch eine Kante verbunden werden sowie Schwellenwerte zum Filtern selten vorkommender Sequenzen ab einer bestimmten Clustergröße, die als Hintergrundrauschen nicht in der Berechnung Berücksichtigung finden sollen. \\

Zur Konstruktion des Graphen wird die Python-Library graph-tool ~\cite{peixoto_2014} genutzt. Die Knoten werden aus den FASTQ-Daten der getrimmten Reads mittels SeqIO aus der Library Biopython ~\cite{cock_2009_1} ausgelesen und im Graphen mit den Knoteneigenschaften ihrer Basensequenz, einer internen ID sowie Angaben zur Basenqualität abgelegt. Die Codierung des Qualitystrings der Reads variiert je nach verwendeter Platform. Daher wird er durch SeqIO ausgelesen und für jede Base in ein einheitliches Maß, den Phred Quality Score $ Q $, decodiert ~\cite{cock_2009_2}. Zusätzlich wird aus den Phred Quality Scores die geschätzte Fehlerwahrscheinlichkeit $ P $ für jede Base nach Formel \eqref{eqn:3-1} bestimmt ~\cite{ewing_1998}.  

\begin{equation} \label{eqn:3-1}
    \tag{3-1}
    P = 10^{\frac{-Q}{10}}
\end{equation}

Für jeden Knoten werden die Vektoren mit den Phred Qualitiy Scores und den geschätzen Fehlerwahrscheinlichkeiten der Basen des Reads als Knoteneigenschaften gespeichert. \\

Die Laufzeit für das Hinzufügen eines Knotens beträgt nach der Dokumentation von graph-tool  $ O(V) $, da es sich hierbei um eine Einfügeoperation in die bereits bestehende Knotenmenge handelt und ein neuer Iterator über alle Knoten erzeugt und zurückgegeben wird ~\cite{docs_graph_tool}. Die Zuweisung der Knoteneigenschaften erfolgt in $ O(1) $. Über alle Reads, also über die resultierende Anzahl der Knoten $ V $ ergibt sich daraus eine Gesamtlaufzeit von $ O(V^2) $.\\

\subsection{Kanten des Graphen} \label{subsec:edges}
Die Kanten des Graphen definieren sich durch das mittels Minimap2 erzeugten Sequenzalignments (vgl. Kap. \ref{sec:edit}). Jedes Alignment zwischen zwei Reads entspricht im Graphen einer gerichteten Kante $e = (source,\; target)$, die den Vergleich der Query- zur Referenzsequenz repräsentiert. Sie verbindet somit zwei der zuvor aus der FASTQ-Daten erzeugten Knoten. Das Auslesen des sam-Formats des Alignmentfiles erfolgt mit Hilfe der Python-Library pysam ~\cite{pysam}. Dabei wird die Edit-Distanz aus dem NM-Tag zunächst genutzt, um nur Kanten in den Graphen aufzunehmen, die bereits einen optimierten Minimap2-Path darstellen. Liegen diese unterhalb des durch die Konfigurationsdatei festgelegten Grenzwertes, so wird die Kante dem Graphen hinzugefügt. Dabei werden als Kanteneigenschaften die Edit-Distanz, die CIGAR-Tupel sowie die aus der Basenqualität und Mutationsrate bestimmte Likelihood hinzugefügt. Zusätzlich kann zur Kontrolle oder für eine spätere Verwendung auch der CIGAR-String selbst als Kanteneigenschaft gespeichert werden, falls bei Minimap2 die Option zur Erzeugung des cs-Tags aktiviert wurde. Die CIGAR-Tupel werden durch pysam aus dem CIGAR-String geparsed, hierbei handelt es sich um eine Liste von Tupeln, die jeweils aus Integer-Wertepaaren bestehen. Der erste Wert jedes Tupels gibt die spezifische Operation des Matches oder Mismatches. So entspricht beispielsweise ein Wert von $ 7 $ oder $ 0 $ einem Match und ein Wert von $ 2 $ einer Deletion. Der zweite Werte jedes Tupels gibt die Anzahl der Basen an, die von der entsprechenden Operation betroffen sind. \\

Diese CIGAR-Tupel werden für die Berechnung der Likelihood zwischen zwei Knoten benötigt, dies erfolgt in der Methode \lstinline|get_alignment_likelihood()| (Algorithmus \ref{alg:lh_read}) aus dem Modul \lstinline|likelihood_operations.py|. Dabei wird aus den p-Werten der Basenqualität für jede Base der Query-Sequenz die Wahrscheinlichkeit errechnet, dass es sich im Falle eines Matches um die korrekte Base handelt  \eqref{eqn:3-2} bzw. im Falle eines Mismatches, dass es sich um einen Sequenzierfehler \eqref{eqn:3-3} oder um eine Mutation handelt \eqref{eqn:3-4}. \\

Die Berechnung der Likelihoods für die paarweisen Vergleiche der Reads basiert auf dem in Kap. ~\ref{subsec:sol_phmm} beschriebenen pair Hidden Markov Model. Hierbei repräsentiert das durch Minimap2 bestimmte Sequenzalignment bereits den wahrscheinlichsten Pfad durch die pairHMM-Matrix. Da dieser Pfad ohnehin die Wahrscheinlichkeit des pairHMM dominieren würde, wird zugunsten der Laufzeit direkt auf das Alignment von Minimap2 zurückgegriffen, um die Likelihoods zwischen den Readsequenzen zu bestimmen. Dabei werden die Sequenzierfehlerrate $ \epsilon $ und Basenqualität $ q_{query} $ durch die bereits zuvor ermittelte geschätzte Fehlerrate $ p_{query} $ berücksichtigt. Die Likelihood $ pairHMM_{\epsilon, q_{query}} \;(s_{ref}\;|\; s_{query}) $, dass der Queryread aus dem Referenzread allein durch Sequenzierfehler und Mutationen entstanden ist, errechnet sich schließlich aus dem Produkt der Likelihoods $ L_{i} $ für jede Base $ b $ an jeder Position $ i $ innerhalb der Sequenz $ s $ des Queryreads $ s_{query} $ im Vergleich zum Referenzread $ s_{ref} $.
\begin{equation} \label{eqn:3-2}
\tag{3-2}
pairHMM_{\epsilon, q_{query}} \;(s_{query}\;|\; s_{ref}) = \prod_{i=1}^{k}L_{i}
\end{equation}

Jede Base $ b $ an Position $ i $ einer Readsequenz $ s $ der Länge $ k $ lässt sich also definieren als $ b \in \{\,b_{i}\in \{A,C,G,T\}^k\;,\; b_{i} \in s \;|\; i = 1, \dotsb, k \,\}$. Seien $ b_{i\,_{ref}} $ und $ b_{i\,_{query}} $ die Basen der Query- und der Referenzsequenzen an Position $ i $ einer Sequenz und $  p_{i\,_{query}} $ die geschätzte Fehlerrate von $ b_{i\,_{query}} $, die sich aus dem Phred Quality Score $ Q $ nach  \eqref{eqn:3-1} ergibt. Seien zudem $ m_{sub} $, $ m_{ins} $ und $ m_{del} $ die über die Konfigurationsdatei festgelegten Mutationsraten für Substitutionen, Insertionen und Deletionen. Dann errechnet sich die Likelihood $ L_{i} = Pr(b_{i\,_{ref}}\;|\; b_{i\,_{query}})$ an der Position $ i $ im Falle eine Matches unter Berücksichtigung der geschätzten Fehlerrate durch:
\begin{equation} \label{eqn:3-3}
\tag{3-3}
L_{i\,_{match}} = 1 - p_{i\,_{query}}
\end{equation}

Bei einem Mismatch dagegen müssen die Wahrscheinlichkeiten von Mutationen und Sequenzierfehlern berücksichtigt werden. Im Falle einer Mutation muss in die Wahrscheinlichkeit eines Matches auch die Mutationsrate des aufgetretenen Mismatches $ m_{rate} \in \{\,m_{sub},\,  m_{ins},\, m_{del}\,\} $ einbezogen werden:
\begin{equation} \label{eqn:3-4}
\tag{3-4}
L_{i\,_{mut}} = m_{rate}\; \cdotp \;(1 - p_{i\,_{query}})
\end{equation}

Die Wahrscheinlichkeit eines Sequenzierfehlers, also dass anstelle der sequenzierten Base tatsächlich eine der drei anderen Basen vorliegt, entspricht $ 1/3 $ der geschätzten Fehlerrate des Phred Quality Scores:
\begin{equation} \label{eqn:3-5}
\tag{3-5}
L_{i\,_{seqerr}} = \frac{1}{3} \; \cdotp \; p_{i\,_{query}}
\end{equation}

Aus \eqref{eqn:3-4} und \eqref{eqn:3-5} errechnet sich also die Likelihood bei einem Mismatch durch:
\begin{equation} \label{eqn:3-6}
\tag{3-6}
L_{i\,_{mismatch}} = (1-m_{rate}) \; \cdotp \; L_{seqerr} \; \cdotp \; L_{mut}
\end{equation}

Aus den Liklihoods von Matches \eqref{eqn:3-3} und Mismatches \eqref{eqn:3-4} kann somit schließlich nach \eqref{eqn:3-2} die Likelihood zwischen den Reads paarweise bestimmt werden.\\

Für eine existierende Kante, von der die CIGAR-Tupel bekannt sind, kann die Methode \lstinline|get_alignment_likelihood()| zudem die Likelihood in entgegengesetzter Richtung bestimmen. Dabei wird der Queryread als Referenzread betrachtet und umgekehrt. Dies ist über das boolsche Argument \lstinline{reverse} steuerbar. Gilt \lstinline|reverse = True|, so werden für die übergebenen CIGAR-Tupel Insertionen zu Deletionen und Deletionen zu Insertionen umbewandelt, anschließend wird die Likelihood nach \eqref{eqn:3-6} berechnet.

Zur zusätzlichen Veranschaulichung ist die Methode \lstinline|get_alignment_likelihood()| in Algorithmus \ref{alg:lh_read} in Pseudocode dargestellt.

\begin{algorithm}[H]
	\caption{Berechnung der Likelihood zwischen zwei Reads}  \label{alg:lh_read}
	\begin{algorithmic}[1]	
		\Function{get\_alignment\_likelihood}{$ m_{sub} $, $ m_{ins} $, $ m_{del} $, $ CIGAR-Tuples $, $ p_{query} $, reverse}
		\State $ likelihood \gets 1.0 $, $ index \gets 0 $
		\If {$reverse$}
		    \State swap values of $ m_{ins} $ and $ m_{del} $
	    \EndIf
	    \ForAll {$ (operation, length) \in CIGAR-Tuples $}
	    \If {$operation \in match $}
		    \While{$ index < length $}
		        \State $ likelihood\, \gets likelihood \,\cdotp (1-p_{query}[index]) $
		    	\State $ index \gets index + 1 $
		    \EndWhile
	    \EndIf
	    \If {$operation \in mismatch $}
	        \State $ m_{rate} \gets 0 $
	        \If {$operation \in substitution $}
	            \State $ m_{rate} \gets m_{sub} $
	        \EndIf
	        \If {$operation \in insertion $}
	            \State $ m_{rate} \gets m_{ins} $
	        \EndIf
	        \If {$operation \in deletion $}
	            \State $ m_{rate} \gets m_{del} $
	        \EndIf
	        \While{$ index < length $}
		        \State $ likelihood\, \gets likelihood \,\cdotp (1 - m_{rate})\,\cdotp \frac{1}{3} \,\cdotp p_{query}[index] \, +  m_{rate}\,\cdotp $         
		         \State \hspace{63pt}  $ (1 - p_{query}[index]) $ 		        
		        \State $ index \gets index + 1 $
	        \EndWhile
	    \EndIf
		\EndFor
		\State \Return $likelihood$
		\EndFunction		
	\end{algorithmic}
\end{algorithm}

Hinsichtlich der Laufzeit benötigt das Hinzufügen einer Kante nach Angaben der graph-tool Dokumentation ~\cite{docs_graph_tool} eine Laufzeit von $ O(1) $. Da aber die Query- und die Referenzreads den bereits zuvor angelegten Knoten zugeordnet werden müssen, erfordert dies eine Suche der betreffenden Knoten. Dabei durchsucht graph-tool mit seiner Funktion \lstinline|find_vertex()| allein die Knoten und prüft auf die gesuchte Read-ID aus den FASTQ-Daten. Die ein- und ausgehenden Kanten der Knoten werden nicht beachtet, so dass eine Tiefen- oder Breitensuche des Graphen nicht notwendig ist und die Suche in $ O(V) $ durchgeführt werden kann ~\cite{graph_tool_coplexity_find_vertex}. Die Zuweisung der Kanteneigenschaften erfolgt jeweils in $ O(1) $, da diese direkt bei der Erzeugung der Kante hinzugefügt werden und keine vorherige Suche der Kante erforderlich ist. Für die Berechnung der Likelihood wird die geschätzte Fehlerrate $ p_{query} $ jeder Base verwendet, so dass die Anzahl der Berechnungen für jede Kante der Länge der Readsequenz $ k $ entspricht. Die Laufzeit für das Hinzufügen einer Kante beträgt somit $ O(k) $. Für alle Kanten ergibt sich daraus eine Gesamtlaufzeit von $ O(E\, \cdotp (k + V)) $. Bei realen Datensätzen gilt in der Regel $ k << V $ und die Länge der Reads variiert nur in einem engen Bereich, so dass $ k $ als vernachlässigbar klein und als nahezu konstant betrachtet werden kann. Dann ergibt sich aus Formel \eqref{eqn:3-7} eine Laufzeit von $O(E\, \cdotp V) $. 
\begin{equation} \label{eqn:3-7}
\tag{3-7}
 O(E\, \cdotp (k + V)) = O(E\, \cdotp V)
\end{equation}

Sind bei kleinen Datensätzen nur wenige Reads vorhanden, so dass $ k \leq V $, dann ergibt sich unter zusätzlicher Berücksichtigung von $k$ im Worst Case mit $ k = V $ nach Formel \eqref{eqn:3-8} ebenfalls eine Laufzeit von $O(E\, \cdotp V) $.
\begin{equation} \label{eqn:3-8}
\tag{3-8}
O(E\, \cdotp (k + V)) = O(E\, \cdotp (V + V)) = O(E\, \cdotp 2 \, \cdotp V) = O(E\, \cdotp V)
\end{equation}

Unter der Annahme, dass in seltenen Fällen die Readlänge, die meist nur wenige hundert Basenpaare zählt, tatsächlich die Anzahl der Reads übersteigt und somit $ k > V $ gilt, dann dominiert $k$ die Laufzeit. In diesem Fall wäre im Worst Case $k$ eine obere Schranke für $V$, so dass gilt:
\begin{equation} \label{eqn:3-9}
\tag{3-9}
O(E\, \cdotp (k + V)) = O(E\, \cdotp (k + k)) = O(E\, \cdotp 2 \, \cdotp k) = O(E\, \cdotp k)
\end{equation}

Da die Länge der Reads durch die gewählten Restriktionsenzyme sowie durch das Sequenzierverfahren selbst beschränkt ist, müsste ein solcher Datensatz relativ klein sein, so dass die damit verbundene Laufzeiterhöhung nur geringfügige Auswirkungen hätte. \\

Zusammenfassend soll daher für die folgenden Berechnungen vereinfachend der Mittelwert der Readlänge als konstant betrachtet werden, also $\overline{k}=const$, so dass für die Berechnung der Likelihoods zwischen den Reads eine Laufzeit von $O(\overline{k}) = O(1)$ veranschlagt wird. Für das Hinzufügen der Kanten des Graphen wird somit die Gesamtlaufzeit auf $ O(E\, \cdotp V) $ geschätzt.\\

Nach Abschluss der Graphkonstruktion werden für jedes Individuum noch einige Statistiken in die Log-Dateien geschrieben. Hier werden neben der Anzahl der Knoten und Kanten des Graphen auch die Anzahl der Substitutionen bzw. SNPs, Insertionen und Deletionen festgehalten, die beim Auslesen der CIGAR-Tupel registriert wurden. Zudem findet sich hier auch die maximal vorkommenden Edit-Distanz über alle Knoten, sofern sich diese unterhalb des festgelegten Schwellenwertes liegt. Ansonsten entspricht sie dem in der Konfigurationsdatei angegebenen Schwellenwert.\\

Als optionaler Output können über die Konfigurationsdatei und die Snakemake-Regel \lstinline|rule noderad| auch die detaillierten Graphinformationen sowie eine Visualisierung des Graphen ausgegeben werden. Die Graphinformationen wie Knoten, Kanten und ihre Eigenschaften können dabei im GraphMl-, DOT-, GML- oder im binären gt-Format gespeichert werden. Die graphische Darstellung wird als pdf-Datei ausgegeben, dabei entspricht die Kantenfärbung der berechneten Likelihood zwischen den Reads.. \\

\subsection{Bestimmung der Zusammenhangskomponenten} \label{subsec:comp}

Die Bestimmung und Indexierung der Zusammenhangskomponenten erfolgt durch graph-tool selbst und kann in $ O(V + E) $ durchgeführt werden ~\cite{docs_graph_tool}. Die Indexnummer jeder Zusammenhangskomponente wird den in ihr enthaltenen Knoten als Knoteneigenschaft hinzugefügt. Zusammenhangskomponenten mit mehr als einem Knoten  werden als neuer eigenständiger Graph initialisiert und in einer Liste abgelegt. Hierfür wird aus dem Graphen für jede Komponente eine gefilterte Sicht erzeugt, die als neues Graph-Object gespeichert wird. Der Filtervorgang jeder Zusammenhangskomponente $ C $ muss für alle Knoten des Graphen durchgeführt werden, daher beträgt die Laufzeit hierfür $ O(C \, \cdotp V) $. Da alle weiteren Schritte des Algorithmus jeweils auf den einzelnen Komponenten durchgeführt werden, kann durch die Verwendung einer Liste von Graphen im Folgenden eine einfachen Iteration über die Komponenten in $ O(C) $ ausgeführt werden, ohne dass der Filtervorgang über alle Knoten jeder Komponente wiederholt werden muss. Zudem ermöglicht diese Datenstruktur eine effizientere Traversierung und Suche innerhalb der Zusammenhangskomponente, ohne dass für jede Komponente der gesamte Graph betrachtet werden muss. Der ursprüngliche Graph wird anschließend entfernt, um Arbeitsspeicher freizugeben. Auch dies erfolgt in konstanter Zeit. Die Laufzeit für die Extraktion der Zusammenhangskomponenten wird also bestimmt durch Identifikation, Indexierung und Filterung der Komponenten mit $ O(C \, \cdotp V) + O(V + E) = O(V \, \cdotp (C + 1) +E)$. Bei realen Daten gibt es in der Regel deutlich mehr Knoten als Cluster bzw. Zusammenhangskomponenten, so dass gilt $ C < V $. Würde im Worst Case aber jede Zusammenhangskomponente aus nur einem Knoten bestehen, also $ C = V $, so kann die maximale Laufzeit auf $ O(V \, \cdotp (C + 1) +E) = O(V \, \cdotp (V + 1) + E) = O(V^2 + E) $ geschätzt werden kann.\\

In der Log-Datei wird die Anzahl der Knoten aller Zusammenhangskomponenten als Histogramm festgehalten. Ebenso wird dort für alle Komponenten mit mehr als einem Element die Anzahl ihrer Knoten, Kanten und Eigenschaften aufgelistet.\\

Über die Konfigurationsdatei und die Snakemake-Regel \lstinline|rule noderad| können optional auch für die Zusammenhangskomponenten jeweils Visualisierungen und detaillierte Graphinformationen in den oben genannten Formaten (Kap. \ref{subsec:edges}) ausgegeben werden. Zudem kann optional auch der gesamte Graph mit den Komponentenindizes als Knoteneigenschaften gespeichert werden. In der visuellen Darstellung werden seine Knoten entsprechend der zugehörigen Zusammenhangskomponente eingefärbt, seine Kantenfärbung richtet sich weiterhin nach der aus \eqref{eqn:3-3} resultierenden Likelihood.

\subsection{Laufzeitanalyse zur Konstruktion des Graphen} \label{subsec:runtime_graph}
Wie an entsprechender Stelle bereits beschrieben, ist für die Erzeugung der Knoten eine Laufzeit von $ O(V^2) $ (Kap. \ref{subsec:nodes}) erforderlich, das Hinzufügen der Kanten kann in $ O(E\, \cdotp V) $ erfolgen. Somit wird für den vollständigen Aufbau des Graphen nach \eqref{eqn:3-10} eine Laufzeit von $ O(V \, \cdotp (V+E)) $ benötigt. \\
\begin{equation} \label{eqn:3-10}
\tag{3-10}
O(V^2) + O(E\, \cdotp V) = O(V \, \cdotp (V+E))
\end{equation}
In Zusammenschau mit der für die Extraktion der Zusammenhangskomponenten erforderliche Laufzeit von $ O(V^2 + E) $ (Kap. \ref{subsec:comp}) ergibt sich daraus nach  \eqref{eqn:3-11} eine Laufzeit von $ O(V \, \cdotp (V + E)) $. 

\begin{equation} \label{eqn:3-11}
\tag{3-11}
\begin{aligned}
&\ {} O(V \, \cdotp (V+E)) +O(V^2 + E) \\
& \ = O(V^2 + E \, \cdotp V + V^2 + E)\\
&\ = O(2 \, \cdotp V^2 + E \, \cdotp (V + 1)) \\
&\ = O(V^2 + E \, \cdotp V)\\
&\ = O(V \, \cdotp (V + E))\\
\end{aligned}
\end{equation}

\section{Bestimmung der maximalen Likelihood der Allele} \label{sec:max_lh_allele}
\subsection{Bestimmung der Allele und ihrer möglichen Häufigkeitsverteilung} \label{subsec:cand_allele}

Die einzelnen Zusammenhangskomponenten repräsentieren einen oder mehrere Loci. Alle weiteren Berechnungen aus diesem und den folgenden Kapiteln ~\ref{sec:max_lh_loci} und ~\ref{sec:vcf} werden für jede Komponenten einzeln durchgeführt mit dem Ziel die wahrscheinlichsten Allelsequenzen und die dazugehörigen Loci zu identifizieren.\\

Zunächst werden mit der Funktion \lstinline|get_candidate_alleles()| aus dem Modul \linebreak \lstinline|likelihood_operations.py| die Allelsequenzen ermittelt, die in der Zusammenhangskomponenten vorkommen. Für deterministischere Ergebnisse werden diese lexikographisch sortiert in Form einer Liste durch die Funktion zurückgegeben. Übersteigt die Größe des Clusters, d.h. die Knotenanzahl einer Zusammenhangskomponenten einen in der Konfigurationsdatei als \lstinline|treshold-cluster-size| festgelegten Wert, so wird von den im Cluster vorkommenden Sequenzen zunächst die absolute Häufigkeit bestimmt. Es werden dann nur diejenigen Sequenzen lexikographisch sortiert zurückgegeben, die einen weiteren, ebenfalls in der Konfigurationsdatei festgelegten Schwellenwert, \lstinline|treshold-seq-noise|, überschreiten. Dies dient dazu, bei großen Clustern Komplexität und Rechenaufwand zu reduzieren, da davon auszugehen ist, dass innerhalb eines großen Clusters echte Varianten einer Sequenz mehrfach vorkommen, wohingegen Artefakte und Sequenzierfehler eher vereinzelt auftreten. Dieses sog. Rauschen kann durch Anpassung der Schwellwerte in der Konfigurationsdatei herausgefiltert werden. Da bei kleineren Clustern auch einmalig registrierte Varianten einer Sequenz von Bedeutung sein können, soll der Filtervorgang erst ab einer festlegbaren Clustergröße durchgeführt werden. Für das Erstellen der Liste muss jeder Knoten in jeder Komponente betrachtet werden, so dass der Vorgang für alle Knoten in $ O(V) $ durchführbar ist.\\

Aus der so erzeugten Liste lexikographisch sortierter Kandidatenallele $ A_{observed} $ der Länge $ n_{observed} $ sollen nun die möglichen Häufigkeitsverteilungen in Abhängigkeit von der Ploidie bestimmt werden. Hierzu muss zunächst die aufgrund der Ploidie tatsächlich zu erwartende Anzahl von Allelen $n_{alleles}$ bestimmt werden. Die geschieht durch die Funktion \lstinline|get_max_parsimony_n_alleles()| in \lstinline|likelihood_operations.py|. Dadurch werden unnötige bzw. nicht mögliche Allelkombinationen eingespart und in der weiteren Berechnung nicht berücksichtigt. Ist die Ploidie $ \phi $ höher als die Anzahl der beobachteten Allele, so muss es mindestens genauso viele und bei Homozygotie auch mehrfach vorkommende Allele geben, damit die Ploidie erfüllt werden kann. Es muss also gelten $ n_{alleles} = \phi $. Wurden dagegen mehr Allele beobachtet als aufgrund der Ploidie möglich sind und die Ploidie ist Teiler von $n_{observed}$, so können alle beobachteten Allele auch tatsächlich vorkommen, da die Zusammenhangskomponente auch mehrere Loci enthalten kann. Es gilt dann also $ n_{alleles} = n_{observed} $. Ist dagegen die Anzahl der beobachteten Allele höher als die Ploidie, aber nicht ganzzahlig durch die Ploidie teilbar, so muss die Anzahl der Allele entsprechend erhöht werden. Das heißt, es müssen tatsächlich so viele Allele vorkommen, dass eine korrekte Ploidie erreicht wird, dass also die Ploidie eine Teiler von $ n_{alleles} $ wird. Die Anzahl der Allele muss also um die Ploidie abzüglich des Restes aus der Restdivision erhöht werden: $ n_{alleles} = n_{observed} + \phi - (n_{observed} \mod \phi)$. Diese Anpassung der Anzahl der Allele kann für jede Komponente $C$ in $ O(1) $ erfolgen, so dass die Laufzeit für alle Komponenten O(C) beträgt. Im Worst Case, bei dem jede Komponente nur einen Knoten enthält und somit $ C = V $ gilt, würde die Laufzeit maximal $ O(V) $ betragen.\\

Mit Hilfe der tatsächlich zu erwarteten Anzahl von Allelen $ n_{alleles} $ können nun alle Kombinationen möglicher Häufigkeitsverteilungen der Allele bestimmt werden (Funktion \linebreak \lstinline|get_candidate_vafs()| im Modul \lstinline|likelihood_operations.py|). Hierfür werden zunächst alle möglichen Allelkombinationen ermittelt. Dies erfolgt nach dem Urnenmodell unter Auswahl von $ n_{alleles} $ Elementen mit Zurücklegen aus insgesamt $ n_{observed} $ verschiedenen Elementen und ohne Berücksichtigung der Reihenfolge. Dadurch berechnet sich die Anzahl möglicher Kombination nach \eqref{eqn:3-12} aus dem  Binomialkoeffizienten der $k$-ten Ordnung aus $ n $ Elementen mit Zurücklegen ~\cite{tb_stat,bronst}.
\begin{equation} \label{eqn:3-12}
\tag{3-12}
\binom{n + k - 1}{k} = \frac{(n+k-1)!}{(n-1)!\, \cdotp k!} = \frac{(n_{alleles}+n_{observed}-1)!}{(n_{alleles}-1)!\, \cdotp n_{observed}!} 
\end{equation}

Diese Allelkombinationen werden mit Hilfe der Funktion  \lstinline|combinations_with_replacement| aus der Python-Library \lstinline|itertools| erzeugt \cite{itertools}.\\
\\
\definecolor{light-gray}{gray}{0.93}
\fcolorbox{black}{light-gray}{
	\parbox{\textwidth}{
		\vspace{0.5cm}
		\textbf{Beispiele möglicher Allelkombinationen:} \\	
		\\	
		$ ploidy = 2, n_{observed} = 2, n_{alleles} = 2$: \\
		{[(0, 0), (0, 1), (1, 1)]}\\
		\\
		$ ploidy = 2, n_{observed} = 3, n_{alleles} = 4$: \\
		{[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 2, 2), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 2, 2), (0, 2, 2, 2), (1, 1, 1, 1), (1, 1, 1, 2), (1, 1, 2, 2), (1, 2, 2, 2), (2, 2, 2, 2)]}
		\\
	}}\\

\subsubsection{Abschätzung der oberenen Laufzeitschranke bei Kombinationen mit Wiederholung}
Kombinationen mit Wiederholung werden sowohl für die Bestimmung der Allelkombinationen aus diesem Kapitel als auch später in Kap. \ref{subsec:comb_loci} für die Ermittlung möglicher Kombinationen von Loci genutzt. Hinsichtlich der Laufzeit und des Speicherplatzbedarfs kann diese Berechnung bei sehr großen Clustern zum dominierenden Faktor werden. Daher soll die Laufzeit an dieser Stelle genauer abgeschätzt werden. Seien dabei für eine bessere Übersichtlichkeit $ n_{alleles} $ als $ n $ und $ n_{observed} $ als $k$ bezeichnet. Nach Formel \eqref{eqn:3-12} werden also $ \binom{n + k - 1}{k} $ Kombinationen jeweils in $O(1)$ erzeugt, d.h. in $ O(\binom{n + k - 1}{k}) $ für jede Zusammenhangskomponente. \\

Die Formel \eqref{eqn:3-12} ergibt sich aus der allgemeinen Formel des Binomialkoeffizienten \eqref{eqn:3-13}, durch den die Anzahl aller Kombinationen ohne Zurücklegen berechnet wird \cite{tb_stat}. 
\begin{equation} \label{eqn:3-13}
\tag{3-13}
\binom{n}{k} = \frac{n!}{(n-k)!\, \cdotp k!}
\end{equation} 
Mit anderen Worten, die Formel \eqref{eqn:3-12} kann auf zwei Arten interpretiert werden: es werden aus n Elementen $ k $ Elemente mit Zurücklegen gezogen oder es werden aus $ n + k - 1 $ Elementen $ k $ Elemente ohne Zurücklegen gezogen. Sei also  $ m = n + k - 1 $, dann gilt:
\begin{equation} \label{eqn:3-14}
\tag{3-14}
\binom{m}{k} = \frac{m!}{(m - k)!\, \cdotp k!} \leq m!
\end{equation} 

Daraus ergibt sich eine Laufzeit zunächst eine obere Schranke der Laufzeit von $ O(m!) = O((n + k)!) $. Hierfür soll nun eine kleinere obere Schranke gefunden werden. Um die Fakultät näherungsweise zu berechnen kann die Stirlingsche Formel \cite{bronst} verwendet werden:
\begin{equation} \label{eqn:3-15}
\tag{3-15}
n! \approx \left( \frac{n}{e} \right) ^n \, \cdotp \sqrt{2 \, \cdotp \pi \, \cdotp n}
\end{equation} 

Somit gilt auch \eqref{eqn:3-16} und ist eine untere Schranke der Fakultät.
\begin{equation} \label{eqn:3-16}
\tag{3-16}
n! \geq \left( \frac{n}{e} \right) ^n 
\end{equation} 

Durch \eqref{eqn:3-12} und \eqref{eqn:3-16} lässt sich durch einige Umformungen die obere Schranke der Laufzeit von ursprünglich $ O(n!) $  auf $ O\left( \left( \frac{e \, \cdotp (n + k - 1)}{k}\right)^k\right) $ eingrenzen:
\begin{equation} \label{eqn:3-17}
\tag{3-17}
\begin{aligned}
 \binom{n + k - 1}{k} &\ {} \overset{\eqref{eqn:3-12}}{=}  \frac{(n+k-1)!}{(n-1)!\, \cdotp k!} = \frac{\prod\limits_{i=1}^{n+k-1}i}{\prod\limits_{i=1}^{n-1}i \; \cdotp \, \prod\limits_{i=1}^{k}i}  \\
&\ = \frac{\prod\limits_{i=n}^{n+k-1}i \; \cdotp \, \prod\limits_{i=1}^{n-1}i}{\prod\limits_{i=1}^{n-1}i \; \cdotp \, \prod\limits_{i=1}^{k}i} = \frac{\prod\limits_{i=n}^{n+k-1}i}{\prod\limits_{i=1}^{k}i}\\
&\ = \frac{(n + k - 1) \, \cdotp (n + k - 2)\, \cdotp \; \dots \; \, \cdotp n}{k!} \\
&\ \leq \frac{(n + k - 1)^k}{k!} \\
&\ \overset{\eqref{eqn:3-16}}{\leq} \frac{(n + k - 1)^k}{\left( \frac{k}{e} \right) ^k} \\
&\ = \left( \frac{e \, \cdotp (n + k - 1)}{k}\right)^k  \\
\end{aligned}
\end{equation} 
Der Binomialkoeffizient verfügt über bestimmte Eigenschaften, insbesondere gelten der Symmetrie- \eqref{eqn:3-18} und der Additionssatz \eqref{eqn:3-19} , die sich auch in der Struktur des Pascalschen Dreiecks widerspiegeln \cite{bronst}. 
\begin{equation} \label{eqn:3-18}
\tag{3-18}
\binom{n}{k} = \binom{n}{n - k}
\end{equation} 
\begin{equation} \label{eqn:3-19}
\tag{3-19}
\binom{n}{k} + \binom{n}{k + 1} = \binom{n + 1}{k + 1} 
\end{equation} 
Aufgrund der Symmetrie sind die Binomialkoeffizienten jeder Zeile des Dreiecks $\binom{n}{k_{i}}$ mit $ k_{i} \in 1, \dots n $ und $ i \in \mathds{N} $ symmetrisch im Bezug auf die Zeilenmitte. Und da nach \eqref{eqn:3-19} jeder Koeffizient aus der Summe der beiden darüber liegenden Koeffizienten entsteht, sind die Werte der Koeffizienten zu den Zeilenrändern hin abnehmend und weisen in der Zeilenmitte ihr Maximum auf. Ist $n$ gerade, so befindet sich die Zeilenmitte bei $ k = \frac{n}{2} $. Ist $n$ ungerade so befinden sich die Maxima bei $ k = \lceil \frac{n}{2} \rceil $ und $ k = \lfloor \frac{n}{2} \rfloor $ und besitzen aufgrund der Symmetrie den gleichen Wert. Vereinfacht gilt also für ein gegebenes $n$, dass der Binomialkoeffizient $\binom{n}{k_{i}}$ bei $ k = \frac{n}{2} $ maximal ist. \\

Daher soll $ k = \frac{n}{2} $ als Worst Case angenommen werden, dann ergibt sich aus \eqref{eqn:3-17} eine Laufzeit von $ O(3e^{\frac{n}{2}} ) \in O(e^n) $. 
	 \begin{equation} \label{eqn:3-xxx1}
\tag{3-xxx1}
\begin{aligned}
\left( \frac{e \, \cdotp (n + k - 1)}{k}\right)^k &\ {} = \left( \frac{e \, \cdotp (n + \frac{n}{2} - 1)}{\frac{n}{2}}\right)^{\frac{n}{2}}  \\
&\ = \left( \frac{\frac{3en - 2e}{2}} {\frac{n}{2}}\right)^{\frac{n}{2}} \\
&\ = \left( \frac{3en - 2e}{n}\right)^{\frac{n}{2}} \\
&\ \leq \left( \frac{3en}{n}\right)^{\frac{n}{2}}\\
&\ = 3e^{\frac{n}{2}}\\
\end{aligned}
\end{equation}
Hinsichtlich der Allelkombinationen entspricht dies einer Zeit von $ O(e^{n_{alleles}}) $ für jede Zusammenhangskomponente. Für die Laufzeit über alle Komponenten $C_{i}$ gilt dann \linebreak $O\left( \sum\limits_{i=1}^{|C|}e^{n_{alleles}}\right) $, es dominiert also die Komponente, welche die meisten Knoten enthält. \\

Für sehr große Cluster ist daher die Laufzeit problematisch. Ebenso kommt es zu einem hohen Bedarf an Arbeitsspeicher, aufgrund der großen Anzahl von Kombinationen die dabei erzeugt werden. Um dies bedingt kompensieren zu können, wurden die oben beschriebenen Schwellwerte eingeführt, die ab einer bestimmten Clustergröße \lstinline|treshold-cluster-size| nur Allelkandidaten ab einer bestimmten Sequenzhäufigkeit in den Reads des Clusters \lstinline|treshold-seq-noise| berücksichtigen.\\

Für jedes Allel $i$ können im Anschluss aus seinen absoluten Häufigkeiten $H_{a_{i}}$ innerhalb jeder Allelkombination die relativen Häufigkeiten $h_{a_{i}}$ nach \eqref{eqn:3-13} bestimmt werden. 
\begin{equation} \label{eqn:3-13}
\tag{3-13}
h_{a_{i}} = \frac{H_{a_{i}}} {n_{alleles}}
\end{equation}
\\		
\fcolorbox{black}{light-gray}{
	\parbox{\textwidth}{
		\vspace{0.5cm}
		\textbf{Beispiele möglicher Häufigkeitsverteilungen der Allele:} \\	
		\\	
		$ ploidy = 2, n_{observed} = 2, n_{alleles} = 2$: \\
		{[1.0, 0.0]}, {[0.5, 0.5]}, {[0.0, 1.0]} \\
		\\
		$ ploidy = 2, n_{observed} = 3, n_{alleles} = 4$: \\
		{[1.0, 0.0, 0.0]}, {[0.75, 0.25, 0.0]}, {[0.75, 0.0, 0.25]}, {[0.5, 0.5, 0.0]}, {[0.5, 0.25, 0.25]}, {[0.5, 0.0, 0.5]}, {[0.25, 0.75, 0.0]}, {[0.25, 0.5, 0.25]}, {[0.25, 0.25, 0.5]}, {[0.25, 0.0, 0.75]}, {[0.0, 1.0, 0.0]}, {[0.0, 0.75, 0.25]}, {[0.0, 0.5, 0.5]}, {[0.0, 0.25, 0.75]}, {[0.0, 0.0, 1.0]}
		\\
}}
\linebreak 
\subsection{Berechnung der Likelihoods der Allele anhand der möglichen Häufigkeitsverteilungen} \label{subsec:lh_allele}
======================= draft =======================\\
für jede Zusammenhangskomponente
2.1 Berechnung der Wslk, einen Read zu beobachten anhand der gegebenen Allele-Fraktion, get\_allele\_likelihood\_read(): Vgl Kap. \ref{subsec:sol_allele_lh}, Formel  \eqref{eqn:2-xxx1} (step 2.1)
\begin{itemize}
	\item für jeden Read soll zunächst die Wslk gesucht werden, dass der Read durch seq-fehler aus einem der Kanditatenallele entstanden ist. 
	\item hierzu werden für jeden Read $ r_{i} $ die ausgehende Kanten gesucht, die jeweils zu jedem der Kandidatenallele $ a_{i} $ führen und die nach \eqref{eqn:3-2} berechnete Likelihood zurückgegeben.
	\item hierbei werden, geordnet nach Laufzeitaufwand verschiedene möglichkeiten geprüft: 
	\item Fall (1) es existiert eine solche Kanten, dann kann die likelihood direkt zurückgegeben werden; 
	\item Fall (2) eine solche Kante existiert nicht, aber es gibt eine entgegengesetzt gerichtete Kanten, dann kann die gesuchte Likelihood des Gegenrichtung, also mit reverese = True über die funktion get\_alignment\_likelihood() bestimmt werden; 
	\item Fall (3) im gesamten Graphen wird nach einer Kante gesucht, die zwischen Knoten verläuft, deren seqenzen denen von $ r_{i} $ und $ a_{i} $ entsprechen. Existiert eine solche Kanten so werden die CIGAR-Tupel dieser Kante verwendet, um für $ r_{i} $ die likelihood zu bestimmen. Da die CIGAR-Tupel allein über die Sequenz definiert werden, gilt das so ermittelte CIGAR-Tupel wegen identischer Sequenzen der begrenzenden Knoten auch für eine nicht existierenden Kante zwischen $ r_{i} $ und $ a_{i} $. Für die Berechnung der Likelihood können diese CIGAR-Tupel also zusammen den p-Werten des Phred Quality Scores (aus \eqref{eqn:3-1}) von $ r_{i} $ die Likelihood mit Hilfe der Funktion get\_alignment\_likelihood() berechnet. 
	\item Fall (4) existiert im gesamten Graphen keine vergleichbare Kante, die $ r_{i} $ und $ a_{i} $ mit einander verbindet, so wird der Wert Null zurückgegeben.
	\item laufzeit:\\	 
	 Laufzeit Kante suchen bei bekannter source und target Knoten $O(\min (d(r_{i}), d(a_{i})))$, mit den Knotengraden $d(r_{i}) $von Knoten $r_{i}$ und $d(a_{i}) $ von Knoten $a_{i}$ ~\cite{docs_graph_tool} -> Worst Case sind alle Knoten des Graphen ausschließlich mit den beiden Knoten $r_{i}$ und $a_{i}$ verbunden, also haben $r_{i}$ und $a_{i}$ jeweils einen Knotengrad, welcher der Anzahl der Knoten des Graphen entspricht mit Ausnahme des Knotens selbst, also : $O(V+\min (d(r_{i}), d(a_{i}))) = O(V+\min (d(V+1), d(V-1))) = O(V+V-1) = O(2 \, \cdotp V) = O(V)$ \\
	 Worst-Case muss die Laufzeit aller Fälle summiert werden:\\
	 Fall (1): über alle out\_neighbors $d(r_{i_{out}})$ des Knotens $r_{i}$ Kante suchen im Worst Case hat $r_{i}$ nur ausgehende Kanten zu allen Knoten des Graphen -> in $ O(d(r_{i_{out}}) \, \cdotp V) = O((V-1) \, \cdotp V) = O(V^2)$, außerdem die Likelihood als Kanteneigenschaft abrufen in $ O(1) $ => also insgesamt $ O(V^2) + O(1)=O(V^2)$\\
	 Fall (2): analog über alle in\_neighbors $d(r_{i_{in}})$ ebenfalls in $O(V^2)$ aber mit Likelihoodberechnung der rückläufigen Kanten in $O(k)$ => insgesamt $ O(V^2 +k) = O(V^2)$, da $\overline{k}=const$ (siehe \ref{subsec:runtime_graph})\\
	 Fall (3): cigar-Tupel finden $O(V)$ (siehe laufzeitberechnung cigar tupel) + likelihoodberechnung $O(k)$ => gesamt: $ O(V +k) = O(V) $\\
	 Fall (4): konstanten Wert zurückgeben in $ O(1) $\\
	 => Gesamt Fall 1 bis 4: \\
	 \begin{equation} \label{eqn:3-xxx1}
	 \tag{3-xxx1}
	 \begin{aligned}
	 &\ {} O(V^2) + O(V^2) + O(V) + O(1))  \\
	 & \ = O(2 \, \cdotp V^2 + V )\\
	 &\ = O(V^2 + V) \\
	 &\ = O(V^2) \\
	 \end{aligned}
	 \end{equation}
	\item pseudocode
	%\begin{algorithm}[H]
	%	\caption{Likelihood zwischen Read und Allel bestimmen}  \label{alg:cig}
	%	\begin{algorithmic}[1]	
	%ToDo -> to pseudocode
	%def get_allele_likelihood_read(comp, allele, node):
	   % # obtian one arbitrary out edge of node that points to another node with 
	   % # sequence = allele
       % out_neighbors = comp.get_out_neighbors(node)
       % in_neighbors = comp.get_in_neighbors(node)
       % # search for existing edge from given node to target node with 
       % # sequence = allele
       % for neighbor in out_neighbors:
       %     if comp.vertex_properties['sequence'][neighbor] == allele:
       %         return comp.edge_properties["likelihood"][comp.edge(node, neighbor)]
       % # search for reverse target node with sequence = allele to given node
       % qual = comp.vp["quality"][node]
       % for neighbor in in_neighbors:
       %     if comp.vertex_properties['sequence'][neighbor] == allele:
       %         return get_alignment_likelihood(comp, 
       %         list(eval(comp.edge_properties['cigar-tuples'][comp.edge(neighbor,  
       %         node)])), qual, reverse=True)
       % cigar_tuples = get_cigar_tuples(comp, comp.vp["sequence"][node], allele)
       % if cigar_tuples:
       %     return get_alignment_likelihood(comp, list(eval(cigar_tuples[0])), qual, 
       %     reverse=cigar_tuples[1])
       % return 0
	%	\end{algorithmic}
	%\end{algorithm}
\end{itemize}
Methode get\_cigar\_tuples():\\
\begin{itemize}
	\item argument sind 2 sequenzen $ s_{source} $ und $ s_{target} $ -> rückgabe CIGAR-Tupel einer kante zwischen source und target und eines boolschen Wertes für das argument 'reverse' der Funktion get\_alignment\_likelihood(), die im anschluss die übergebenenen CIGAR-Tupel zur likelihood berechnung verwendet. Verläuft die Kante von $ s_{source} $ zu $ s_{target} $ so ist dieser Wert False, verläuft sie dagegen in Gegenrichtung so gilt reverse = True. \\
    \item Algorithmus:
	\item 1. im Graphen Menge aller Knoten suchen $ S_{source} $, die die sequenz $ s_{source} $ besitzen $O(V)$ (vgl. Kap. \ref{subsec:edges}) \\
	\item 2. im Graphen Menge aller Knoten suchen $ S_{target} $, die die sequenz $ s_{target} $ besitzen $ O(V) $\\
	\item 3. im Graphen eine Kante suchen die einen der Knoten $v$ aus $ S_{source} $ mit einem der Knoten $w$ aus $ S_{target} $ verbindet. Falls eine solche Kanten existiert, dann werden die CIGAR-Tupel dieser Kanten sowie der boolsche Wert False zurückgegeben. $O(\min (d(v), d(w)))$, mit den Knotengraden $d(v) $von Knoten $v$ und $d(w) $ von Knoten $w$ ~\cite{docs_graph_tool}\\
	\item 4. war die Suche aus 3. erfolglos, so wird eine Kante in Gegenrichtung gesucht, d.h. eine Kante die zwischen einem Knoten aus $ S_{target} $ zu einem Knoten aus $ S_{source} $ verläuft. Wurde eine solche Kanten gefunden, so werden ihre CIGAR-Tupel zusammen mit dem boolschen Wert True zurückgegeben $O(\min (d(v), d(w)))$.\\
	\item 5. Existiert im gesamten Graphen keine Kante, die  $ s_{source} $ und $ s_{target} $ verbindet, so wird None zurückgegeben. O(1)\\
	\item -> gesamt $ O(V+\min (d(v), d(w)+\min (d(v), d(w)) = O(V+\min (d(v), d(w))$	\\
	\item -> pseudocode\\

	%\begin{algorithm}[H]
	%	\caption{CIGAR-Tupel bestimmen}  \label{alg:cig}
	%	\begin{algorithmic}[1]	
		%ToDo -> to pseudocode
		%def get_cigar_tuples(comp, seq, allele):
		%    source_nodes = [comp.vertex_index[node] for node in
		%    find_vertex(comp, comp.vp["sequence"], seq)]
		%    target_nodes = [comp.vertex_index[node] for node in find_vertex(comp, comp.vp["sequence"], allele)]
		%    for node_s in source_nodes:
		%        for node_t in target_nodes:
		%            edge = comp.edge(node_s, node_t)
		%            if edge:
		%                return comp.edge_properties["cigar-tuples"][edge], False
		%            rev_edge = comp.edge(node_t, node_s)
		%            if rev_edge:
		%                return comp.edge_properties["cigar-tuples"][rev_edge], True
		%    return None
	%	\end{algorithmic}
	%\end{algorithm}
\end{itemize}

2.2 Summe der Wslk über die Häufigkeiten aller Kandidatenallele in einer vaf-kombi für 1 read im Worst Case $O(n + \phi - (n \mod \phi))$ mit $n$ Anzahl der Allele und $\phi$ ploidie; Formel \eqref{eqn:2-xxx2} calc\_vafs\_likelihood\_read()\\ 

2.3 Produkt für alle Reads $O(V)$; calc\_vafs\_likelihood(); Formel \eqref{eqn:2-xxx3}\\ 

2.4 schritte 2.1 bis 2.3 für alle vafs/für alle Kombinationsmöglichkeiten der Allele bestimmen und daraus => Maximum \\

=> gesamtlaufzeit:\\
\begin{itemize}
	\item da jeder Schritt 2.1 bis 2.4 komponentenweise berechnet werden, d.h. jeder Knoten und jede Kante jeder Komponente betrachtet wird, entspricht dies allen Knoten und Kanten des Graphen, also wird die Laufzeitberechnung zusammenfassend auf allen Knoten und Kanten berechnet mit $O(\sum_{i=1}^{|C|} C_{i} \, \cdotp V_{c_{i}})=O(V) $ und $O(\sum_{i=1}^{|C|} C_{i} \, \cdotp E_{c_{i}})=O(E) $ mit $C$ Menge der Zusammenhangskomponenten, $C_{i}$ ist $i$-te Zusammenhangskomponenten, $V_{c_{i}}$ Anzahl der Knoten innerhalb der Komponente $C_{i}$ und $E_{c_{i}} $ Anzahl der Kanten innerhalb der Komponente  $C_{i}$
	\item step 2.1: $O(V^2)$\\
	\item step 2.2: im Worst Case hat jeder Knoten eine einzigartige sequenz, dann entspricht die Anzahl der Allele der Anzahl der Knoten, also $n = V$, die Laufzeit kann also maximal $O(n + \phi - (n \mod \phi) = O(V+\phi-1)= O(V+\phi)) = O(V)$ betragen, da $\phi = const$ gilt $ O(V+\phi)) = O(V) $
	\item insgesamt ergibt sich also aus 2.1 bis 2.3 eine Laufzeit von:\\
    \begin{equation} \label{eqn:3-xxx2}
	\tag{3-xxx2}
	\begin{aligned}
	&\ {} O(V) \, \cdotp (O(V) + O(V^2)\\
	&\ = O(V^2 + V^3) \\
	&\ = O(V^3)\\
	\end{aligned}
	\end{equation}
\end{itemize}

in den log-files werden für jede Komponente die anzahl der allele, die ploidie und die Häufigkeitsverteilung aus der vaf mit der maximalen likelihood mit den dazugehörigen allelsequenzen festgehalten.


\section{Bestimmung der maximalen Likelihood der Loci} \label{sec:max_lh_loci}
Für jede Zusammenhangskomponente\\
jede zusammenhangskomponente kann auch mehr als nur einen Locus beinhalten.\\
\subsection{Bestimmung der möglichen Loci} \label{subsec:comb_loci}
ermittlung der möglichen loci-Kombinationen erfolgt in der Funktion get\_candidate\_loci() analog zur Funktion get\_candidate\_vafs() (vgl. Kap \ref{subsec:cand_allele}):
\begin{itemize}
	\item auch hier ist die bereits beschriebene anzahl der tatsächlich zu erwartenden allele $n_{alleles}$ aus get\_max\_parsimony\_n\_alleles() grundlage für die berechnung der möglichen loci-Kombinationen 
	\item anschließend auch hier durch combinations\_with\_replacement (python-library itertools) alle Kombinationen mit wiederholung für $n_{alleles} $ bestimmen -> mehrfaches vorkommen erlaubt, reihenfolge irrelevant 
	\item die daraus resultierenden Allel-Kombinationen werden denn aber mit Hilfe der Funktion grouper (aus den itertools recipes der python-library itertools) zu gruppen entsprechend der in der Konfigurationsdatei angegebenen ploidie zusammengefasst.
	\item dadurch entstehen verschiedene Kombinationen von möglichen loci als listen von Tupeln:\\
		Bsp.: \\
	    $ n_{alleles} = 2, ploidy = 2$: \\
	    {[(0, 0)]}, {[(0, 1)]}, {[(1, 1)]} \\
	\item falls aufgrund der erwarteten tatsächlichen anzahl der allele $n_{alleles} $ mehrere Loci möglich sind, resultieren Kombinationen von Loci:\\
	    $ n_{alleles} = 3, ploidy = 2$: \\
	    {[(0, 0), (0, 0)]}, {[(0, 0), (0, 1)]}, {[(0, 0), (0, 2)]}, {[(0, 0), (1, 1)]}, {[(0, 0), (1, 2)]}, {[(0, 0), (2, 2)]}, {[(0, 1), (1, 1)]}, {[(0, 1), (1, 2)]}, {[(0, 1), (2, 2)]}, {[(0, 2), (2, 2)]}, {[(1, 1), (1, 1)]}, {[(1, 1), (1, 2)]}, {[(1, 1), (2, 2)]}, {[(1, 2), (2, 2)]}, {[(2, 2), (2, 2)]} \\
	\item jedes Tupel der Länge der ploidie entspricht also einem möglichen Locus
\end{itemize}

\subsection{Zuordnung der wahrscheinlichsten Allelverteilung zu passenden Loci} \label{subsec:lh_loci}
Für jede mögliche Kombination der Loci  aus get\_candidate\_loci() wird zunächst durch die Indikatorfunktion indicator\_constrait() nach Formel \eqref{eqn:2-xxx4} geprüft, ob sich die Loci dieser Kombination der zuvor bestimmten vaf mit maximaler Likelihood, also der wahrscheinlichsten Allelverteilung, zuordnen lassen. Ist die Bedingung der Indikatorfunktion erfüllt, so wird die Heterozygotiewahrscheinlichkeit für die allele der loci dieser Kombination berechnet und zurückgegeben. Ist die Bedingung der Indikatorfunktion nicht erfüllt, so wird eine Likelihood von 0 zurückgegeben.

Trifft die Bedingung der Indikatorfunktion zu, so folgt die Berechnung der wahrscheinlichsten Zuordnung der Loci zu den Allelen aus den max. Likelihood vafs. Dafür werden der Loci-Kombination zunächst die dazugehörigen allelsequenzen zugeordnet. Die allel-nummer in einer Loci-Kombination entspricht dabei dem index der liste der allelsequenzen der kandidaten-allele aus get\_candidate\_alleles() (siehe Kap. \ref{subsec:cand_allele}).

Analog zur Likelihoodberechnung beim paarweisen Vergleich der Reads unter Berücksichtigung der Sequenzierfehlerwahrscheinlichkeit \ref{subsec:edges} erfolgt auch die Berechnung der Likelihood für eine bestimmte Loci-Kombination durch den paarweisen Vergleich der Allele im Sinne eines pair Hidden Markov Models $ pairHMM_{\eta}(a_{l_{j,1}}, a_{l_{j,2}}) $ (vgl. Kap. ~\ref{subsec:sol_phmm}). Allerdings erfolgt nun der Vergleich unter Berücksichtigung der Heterozygotiewahrscheinlichkeit nach Formel \eqref{eqn:2-xxx5}. 

Durch die Funktion get\_allel\_linkelihood\_allele() werden dabei die Allelsequenzen der Loci-Kombination paarweise miteinander verglichen. Hierfür wird für jedes Paar der CIGAR-String durch die in Kap. ~\ref{subsec:lh_allele} bereits beschriebene Funktion get\_cigar\_tuples() ermittelt.

Der CIGAR-String wird anschließend dafür benötigt aus den paarweisen Kombinationen der Allelsequenzen innerhalb eines Locus die Likelihood im Hinblick auf die Heterozygotiewahrscheinlichkeit zu berechnen. Analog zum Algorithmus ~\ref{alg:lh_read} wird in der Funktion  get\_heterozygosity() die Likelihood über die Heterozygotiewahrscheinlichkeiten aller Matches und Mismatches berechnet.

Dabei entspricht im Falle eines Mismatches die Likelihood der in der Konfigurationsdatei angegebenen Heterozygotiewahrscheinlichkeit $ \eta $ für die betreffende Mutationsart, also $ \eta_{sub} $ für Substitutionen , $ \eta_{ins} $ für Insertionen bzw. $ \eta_{del} $ Deletionen. Sei $i$ der Index der betreffenden Base innerhalb der betrachteten Allelsequenz und $ \eta_{rate} \in \{\,\eta_{sub},\, \eta_{ins},\, \eta_{del}\,\}$, dann berechnet sich bei einem Mismatch die Likelihood $L_{i}$ der Base nach Formel \eqref{eqn:3-xxx3}.
\begin{equation} \label{eqn:3-xxx3}
\tag{3-xxx3}
L_{i\,_{mismatch}} = \eta_{rate}
\end{equation}

Im Falle eines Matches senkt die Möglichkeit eines Mismatches die Likelihood der betreffenden Base entsprechend um die Summe der Heterozygotiewahrscheinlichkeiten der genannten Mutationsarten (Formel \eqref{eqn:3-xxx4}).
\begin{equation} \label{eqn:3-xxx4}
\tag{3-xxx4}
L_{i\,_{match}} = 1 - (\eta_{sub} + \eta_{ins} + \eta_{del})
\end{equation}

Die Likelihood $ Pr(T=a_{l_{j,2}} \, | \, S=a_{l_{j,1}}, \eta) $ des paarweisen Vergleichs der Allele $a_{l_{j,1}}$ und $a_{l_{j,2}}$ hinsichtlich der Zuordnung zu den Loci-Kombinationen errechnet sich schließlich aus dem Produkt der Wahrscheinlichkeiten der einzelnen Basen:
\eqref{eqn:3-xxx5}).
\begin{equation} \label{eqn:3-xxx5}
\tag{3-xxx5}
Pr(T=a_{l_{j,2}} \, | \, S=a_{l_{j,1}}, \eta) = pairHMM_{\eta}(a_{l_{j,1}}, a_{l_{j,2}}) = \prod_{i=1}^{k}L_{i}
\end{equation}


komponentenweise mit $C = V$ im Worst Case (Kap. \ref{subsec:comp}) -> $O(V)$

\section{Ausgabe der wahrscheinlichsten Loci als VCF-Datei} \label{sec:vcf}

Ausgabe im VCF-Format, tab-separierter body\\
Als Header der Spalten: CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMA, <Name der sample>\\
get\_sorted\_loci\_alleles(): lexikographisch sortierte Liste der Menge (d.h. ohne Duplikate) der Allelsequenzen der loci-Kombination mit max. Likelihood $A_{opt}$-> der erste Eintrag wird in die REF-Spalte des VCF-Files eingetragen, alle übrigen kommasepariert in die ALT-Spalte\\

Da die vollständige Sequenz der Allele angegeben wird ist immer POS=1
CHROM wird für die Bennennung der Loci genutzt im Format LOC<ldf.-Nr>
ID, QUAL, FILTER und INFO werden hier nicht genutzt und mit einem dot als Platzhalter bei allen Loci befüllt\\
FORMAT für alle Loci wird der Genotype-Datentyp ''GT'' angegeben, dieser wird in der Spalte der sample spezifiziert. Da die Indizes der Loci der gesamten Liste der Kandidaten-Allele $A_{cand}$ zugeordnet sind, muss ihre Indizierung nun auf $A_{opt}$ angepasst werden.  Dafür werden in get\_alleles\_matched\_to\_loci() zunächst jedem Locus der max Likelihood Loci-Kombination die dazugehörigen Allelsequenzen zugeordnet. Diese werden anschließend in get\_gt\_indices() den Indizes der entsprechenden Sequenz aus $A_{opt}$ zugeordnet, so dass das lexikographisch erste Allel, also REF mit 0 im Genotyp indiziert ist, die Allele aus ALT entsprechend mit höheren Indizes entsprechend ihrer Sortierung in $A_{opt}$. Nun lässt sich der Genotyp im Bezug auf die Sequenzen in REF und ALT aus diesen Indizes und getrennt durch Slashes direkt angeben. Dies geschieht in der Funktion get\_genotype(), deren Ergebnis dann in das sample-Feld eingetragen wird.\\
