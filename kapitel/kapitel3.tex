% kapitel3.tex
\chapter{Algorithmus} \label{sec:alg}
Für das hier implementierte RAD-Sequencing-Tool, NodeRAD, wurde zur Workflowintegration das Workflow Management System Snakemake verwendet ~\cite{koester_2012_1, koester_2012_2}. Die einzelnen Analyseschritte werden dabei über Regeln abgebildet. Für jede Regel können neben dem zu verwendenden Script oder Shell-Kommando sowie den Pfadangaben für In- und Output auch zusätzliche Optionen festgelegt werden. Dazu gehören beispielsweise Angaben zu Parametern bzw. Argumenten für die verwendeten Tools, Pfadangaben für Log-Dateien oder die Anzahl der zu verwendenden Threads.

Als Input benötigt der Workflow eine Datei im FASTQ-Format, welche die single-end Reads der verschiedenen Individuen mit ihren Identifikationsbezeichnungen, der Basensequenz und Angaben zur  Basenqualität enthält. Des Weiteren wird eine Tabelle im tsv-Format benötigt, in der die Zuordnung der Probennamen zu den Individuen und ihren Barcode-Sequenzen definiert ist. Nach dem Preprocessing, der Qualitätskontrolle der Reads und dem Sequence-Alignment erfolgt die RAD-Seq-Analyse durch NodeRAD. Hierbei werden die Wahrscheinlichkeiten der Allelsequenzen und der möglichen Loci bestimmt. Die Loci mit der höchsten Wahrscheinlichkeit werden schließlich mit den Sequenzen ihrer Allele und den möglichen Varianten entsprechend dem ermittelten Genotyp in einer Datei im Variant Call Format (VCF) ausgegeben.

\section{Preprocessing} \label{sec:preproc}

Im Preprocessing werden durch das Tool Cutadapt ~\cite{martin_2011} die Reads jedes Individuums anhand ihrer Barcodesequenzen identifiziert und extrahiert (Demultiplexing). Hiernach werden die Barcodesequenzen entfernt (Trimming) und die Reads jedes Individuums in separaten Dateien im FASTQ Format abgelegt. \\
Im Anschluss an das Trimming erfolgt eine Qualitätskontrolle durch das Tool FastQC  ~\cite{andrews_2012}. Dabei werden einige allgemeine Statistiken zu den Rohdaten der Reads generiert, wie beispielsweise zur Basenqualität, zum GC-Gehalt, dem Anteil an Duplikaten oder überrepräsentierten Sequenzen. Durch das Tool MultiQC ~\cite{ewels_2016} wird aus diesen Statistiken und den Log-Dateien von Cutadapt ein html-Report mit diversen Plots zur Veranschaulichung erstellt.

\section{Edit-Distanzen} \label{sec:edit}
Für die spätere Konstruktion eines Graphen basierend auf den Edit-Distanzen zwischen den Readsequenzen wird für jedes Individuum zunächst ein Sequenzalignment mit Hilfe des Tools Minimap2 ~\cite{li_2018} erstellt. Hierbei werden alle Readsquenzen paarweise verglichen und in Abhängigkeit von ihren Übereinstimmungen (Matches) und Unterschieden (Mismatches) einander zugeordnet. Das Ergebnis des Mappings wird im sam-Format ~\cite{li_2009} ausgegeben und enthält Angaben zur betrachteten Sequenz (Query), die gegen einen anderen Read (Reference) verglichen wurde. Neben den ID's der Query- und Reference-Sequenzen, werden dort unter anderem auch der CIGAR-String, Informationen zur Basenqualität der Query-Sequenz, sowie optional verschiedene Tags angegeben. Ein für die späteren Berechnungen wichtiges Maß sind die Edit-Distanzen, die durch den NM-Tag repräsentiert werden. Die Edit-Distanz gibt hierbei die minimale Anzahl von Editieroperationen an, um die Query-Sequenz in die Referenzsequenz zu transformieren. Als Editieroperationen sind hierbei ersetzen, einfügen und löschen von Basen möglich. Auf DNA-Ebene entspricht dies den Punktmutationen im Sinne von Substitutionen, Insertionen und Deletionen (vgl. Kap. \ref{subsec:mutation}). Der CIGAR-String ist eine kondensierte Darstellung der Unterschiede zwischen Query- und Reference-Sequenz. In ihm werden Matches und Mismatches wie Insertionen, Substitutionen und Deletion jeweils mit der Anzahl der betroffenen Basen angegeben. Sowohl der CIGAR-String als auch der NM-Tag definieren wichtige Kanteneigenschaften des späteren Graphen. \\

\section{Konstruktion des Graphen} \label{sec:graph}
\subsection{Knoten des Graphen} \label{subsec:nodes}
Das hier in Python implementierte Tool, NodeRAD, benötigt als Input zu jedem Individuum die getrimmten single-end Read-Daten sowie das Sequenzalignment. Zunächst wird daraus für jedes Individuum ein eigener, gerichteter Graph $ G $ mit $ G = (V,E) $ erstellt. Seine Knoten, $ V $, werden durch die einzelnen Reads repräsentiert. Entsprechend ergeben sich die Knoteneigenschaften aus den Daten der Reads, diese werden den FASTQ-Dateien nach Ausführung von Cutadapt (siehe \ref{sec:preproc}) entnommen. Die Kanten, $ E $, zwischen den Knoten ergeben sich aus dem Vergleich ihrer Sequenzen im Rahmen des Sequenzalignments mittels Minimap2 (siehe \ref{sec:edit}).\\

Zusätzlich entnimmt NodeRAD der Konfigurationsdatei des Workflows einige Konstanten und Grenzwerte für die späteren Berechnungen. Dazu gehören die Mutationsraten und Heterozygotiewahrscheinlichkeiten für Substitutionen, Insertionen und Deletionen, die Ploidie des Chromosomensatzes der untersuchten Spezies und Grenzwerte. Die Konstanten werden als Grapheigenschaften im Graphen abgelegt. Als konfigurierbare Grenzwerte gibt es für NodeRAD einen Schwellenwert für die maximal zulässige Editierdistanz, bei dem zwei Knoten noch durch eine Kante verbunden werden sowie Schwellenwerte zum Filtern selten vorkommender Sequenzen ab einer bestimmten Clustergröße, die als Hintergrundrauschen nicht in der Berechnung Berücksichtigung finden sollen. \\

Zur Konstruktion des Graphen wird die Python-Library graph-tool ~\cite{peixoto_2014} genutzt. Die Knoten werden aus den FASTQ-Daten der getrimmten Reads mittels SeqIO aus der Library Biopython ~\cite{cock_2009_1} ausgelesen und im Graphen mit den Knoteneigenschaften ihrer Basensequenz, einer internen ID sowie Angaben zur Basenqualität abgelegt. Die Codierung des Qualitystrings der Reads variiert je nach verwendeter Platform. Daher wird er durch SeqIO ausgelesen und für jede Base in ein einheitliches Maß, den Phred Quality Score $ Q $, decodiert ~\cite{cock_2009_2}. Zusätzlich wird aus den Phred Quality Scores die geschätzte Fehlerwahrscheinlichkeit $ P $ für jede Base nach Formel \eqref{eqn:3-1} bestimmt ~\cite{ewing_1998}.  

\begin{equation} \label{eqn:3-1}
    \tag{3-1}
    P = 10^{\frac{-Q}{10}}
\end{equation}

Für jeden Knoten werden die Vektoren mit den Phred Qualitiy Scores und den geschätzen Fehlerwahrscheinlichkeiten der Basen des Reads als Knoteneigenschaften gespeichert. \\

Die Laufzeit für das Hinzufügen eines Knotens beträgt nach der Dokumentation von graph-tool  $ O(V) $, da es sich hierbei um eine Einfügeoperation in die bereits bestehende Knotenmenge handelt und ein neuer Iterator über alle Knoten erzeugt und zurückgegeben wird ~\cite{docs_graph_tool}. Die Zuweisung der Knoteneigenschaften erfolgt in $ O(1) $. Über alle Reads, also über die resultierende Anzahl der Knoten $ V $ ergibt sich daraus eine Gesamtlaufzeit von $ O(V^2) $.\\

\subsection{Kanten des Graphen} \label{subsec:edges}
Die Kanten des Graphen definieren sich durch das mittels Minimap2 erzeugten Sequenzalignments (vgl. Kap. \ref{sec:edit}). Jedes Alignment zwischen zwei Reads entspricht im Graphen einer gerichteten Kante $e = (source,\; target)$, die den Vergleich der Query- zur Referenzsequenz repräsentiert. Sie verbindet somit zwei der zuvor aus der FASTQ-Daten erzeugten Knoten. Das Auslesen des sam-Formats des Alignmentfiles erfolgt mit Hilfe der Python-Library pysam ~\cite{pysam}. Dabei wird die Edit-Distanz aus dem NM-Tag zunächst genutzt, um nur Kanten in den Graphen aufzunehmen, die bereits einen optimierten Minimap2-Path darstellen. Liegen diese unterhalb des durch die Konfigurationsdatei festgelegten Grenzwertes, so wird die Kante dem Graphen hinzugefügt. Dabei werden als Kanteneigenschaften die Edit-Distanz, die CIGAR-Tupel sowie die aus der Basenqualität und Mutationsrate bestimmte Likelihood hinzugefügt. Zusätzlich kann zur Kontrolle oder für eine spätere Verwendung auch der CIGAR-String selbst als Kanteneigenschaft gespeichert werden, falls bei Minimap2 die Option zur Erzeugung des cs-Tags aktiviert wurde. Die CIGAR-Tupel werden durch pysam aus dem CIGAR-String geparsed, hierbei handelt es sich um eine Liste von Tupeln, die jeweils aus Integer-Wertepaaren bestehen. Der erste Wert jedes Tupels gibt die spezifische Operation des Matches oder Mismatches. So entspricht beispielsweise ein Wert von $ 7 $ oder $ 0 $ einem Match und ein Wert von $ 2 $ einer Deletion. Der zweite Werte jedes Tupels gibt die Anzahl der Basen an, die von der entsprechenden Operation betroffen sind. \\

Diese CIGAR-Tupel werden für die Berechnung der Likelihood zwischen zwei Knoten benötigt, dies erfolgt in der Methode \lstinline|get_alignment_likelihood()| (Algorithmus \ref{alg:lh_read}) aus dem Modul \lstinline|likelihood_operations.py|. Dabei wird aus den p-Werten der Basenqualität für jede Base der Query-Sequenz die Wahrscheinlichkeit errechnet, dass es sich im Falle eines Matches um die korrekte Base handelt  \eqref{eqn:3-2} bzw. im Falle eines Mismatches, dass es sich um einen Sequenzierfehler \eqref{eqn:3-3} oder um eine Mutation handelt \eqref{eqn:3-4}. \\

Die Berechnung der Likelihoods für die paarweisen Vergleiche der Reads basiert auf dem in Kap. ~\ref{subsec:sol_phmm} beschriebenen pair Hidden Markov Model. Hierbei repräsentiert das durch Minimap2 bestimmte Sequenzalignment bereits den wahrscheinlichsten Pfad durch die pairHMM-Matrix. Da dieser Pfad ohnehin die Wahrscheinlichkeit des pairHMM dominieren würde, wird zugunsten der Laufzeit direkt auf das Alignment von Minimap2 zurückgegriffen, um die Likelihoods zwischen den Readsequenzen zu bestimmen. Dabei werden die Sequenzierfehlerrate $ \epsilon $ und Basenqualität $ q_{query} $ durch die bereits zuvor ermittelte geschätzte Fehlerrate $ p_{query} $ berücksichtigt. Die Likelihood $ pairHMM_{\epsilon, q_{query}} \;(s_{ref}\;|\; s_{query}) $, dass der Queryread aus dem Referenzread allein durch Sequenzierfehler und Mutationen entstanden ist, errechnet sich schließlich aus dem Produkt der Likelihoods $ L_{i} $ für jede Base $ b $ an jeder Position $ i $ innerhalb der Sequenz $ s $ des Queryreads $ s_{query} $ im Vergleich zum Referenzread $ s_{ref} $.
\begin{equation} \label{eqn:3-2}
\tag{3-2}
pairHMM_{\epsilon, q_{query}} \;(s_{query}\;|\; s_{ref}) = \prod_{i=1}^{k}L_{i}
\end{equation}

Jede Base $ b $ an Position $ i $ einer Readsequenz $ s $ der Länge $ k $ lässt sich also definieren als $ b \in \{\,b_{i}\in \{A,C,G,T\}^k\;,\; b_{i} \in s \;|\; i = 1, \dotsb, k \,\}$. Seien $ b_{i\,_{ref}} $ und $ b_{i\,_{query}} $ die Basen der Query- und der Referenzsequenzen an Position $ i $ einer Sequenz und $  p_{i\,_{query}} $ die geschätzte Fehlerrate von $ b_{i\,_{query}} $, die sich aus dem Phred Quality Score $ Q $ nach  \eqref{eqn:3-1} ergibt. Seien zudem $ m_{sub} $, $ m_{ins} $ und $ m_{del} $ die über die Konfigurationsdatei festgelegten Mutationsraten für Substitutionen, Insertionen und Deletionen. Dann errechnet sich die Likelihood $ L_{i} = Pr(b_{i\,_{ref}}\;|\; b_{i\,_{query}})$ an der Position $ i $ im Falle eine Matches unter Berücksichtigung der geschätzten Fehlerrate durch:
\begin{equation} \label{eqn:3-3}
\tag{3-3}
L_{i\,_{match}} = 1 - p_{i\,_{query}}
\end{equation}

Bei einem Mismatch dagegen müssen die Wahrscheinlichkeiten von Mutationen und Sequenzierfehlern berücksichtigt werden. Im Falle einer Mutation muss in die Wahrscheinlichkeit eines Matches auch die Mutationsrate des aufgetretenen Mismatches $ m_{rate} \in \{\,m_{sub},\,  m_{ins},\, m_{del}\,\} $ einbezogen werden:
\begin{equation} \label{eqn:3-4}
\tag{3-4}
L_{i\,_{mut}} = m_{rate}\; \cdotp \;(1 - p_{i\,_{query}})
\end{equation}

Die Wahrscheinlichkeit eines Sequenzierfehlers, also dass anstelle der sequenzierten Base tatsächlich eine der drei anderen Basen vorliegt, entspricht $ 1/3 $ der geschätzten Fehlerrate des Phred Quality Scores:
\begin{equation} \label{eqn:3-5}
\tag{3-5}
L_{i\,_{seqerr}} = \frac{1}{3} \; \cdotp \; p_{i\,_{query}}
\end{equation}

Aus \eqref{eqn:3-4} und \eqref{eqn:3-5} errechnet sich also die Likelihood bei einem Mismatch durch:
\begin{equation} \label{eqn:3-6}
\tag{3-6}
L_{i\,_{mismatch}} = (1-m_{rate}) \; \cdotp \; L_{seqerr} \; \cdotp \; L_{mut}
\end{equation}

Aus den Liklihoods von Matches \eqref{eqn:3-3} und Mismatches \eqref{eqn:3-4} kann somit schließlich nach \eqref{eqn:3-2} die Likelihood zwischen den Reads paarweise bestimmt werden.\\

Für eine existierende Kante, von der die CIGAR-Tupel bekannt sind, kann die Methode \lstinline|get_alignment_likelihood()| zudem die Likelihood in entgegengesetzter Richtung bestimmen. Dabei wird der Queryread als Referenzread betrachtet und umgekehrt. Dies ist über das boolsche Argument \lstinline{reverse} steuerbar. Gilt \lstinline|reverse = True|, so werden für die übergebenen CIGAR-Tupel Insertionen zu Deletionen und Deletionen zu Insertionen umbewandelt, anschließend wird die Likelihood nach \eqref{eqn:3-6} berechnet.

Zur zusätzlichen Veranschaulichung ist die Methode \lstinline|get_alignment_likelihood()| in Algorithmus \ref{alg:lh_read} in Pseudocode dargestellt.

\begin{algorithm}[H]
	\caption{Berechnung der Likelihood zwischen zwei Reads}  \label{alg:lh_read}
	\begin{algorithmic}[1]	
		\Function{get\_alignment\_likelihood}{$ m_{sub} $, $ m_{ins} $, $ m_{del} $, $ CIGAR-Tuples $, $ p_{query} $, reverse}
		\State $ likelihood \gets 1.0 $, $ index \gets 0 $
		\If {$reverse$}
		    \State swap values of $ m_{ins} $ and $ m_{del} $
	    \EndIf
	    \ForAll {$ (operation, length) \in CIGAR-Tuples $}
	    \If {$operation \in match $}
		    \While{$ index < length $}
		        \State $ likelihood\, \gets likelihood \,\cdotp (1-p_{query}[index]) $
		    	\State $ index \gets index + 1 $
		    \EndWhile
	    \EndIf
	    \If {$operation \in mismatch $}
	        \State $ m_{rate} \gets 0 $
	        \If {$operation \in substitution $}
	            \State $ m_{rate} \gets m_{sub} $
	        \EndIf
	        \If {$operation \in insertion $}
	            \State $ m_{rate} \gets m_{ins} $
	        \EndIf
	        \If {$operation \in deletion $}
	            \State $ m_{rate} \gets m_{del} $
	        \EndIf
	        \While{$ index < length $}
		        \State $ likelihood\, \gets likelihood \,\cdotp (1 - m_{rate})\,\cdotp \frac{1}{3} \,\cdotp p_{query}[index] \, +  m_{rate}\,\cdotp $         
		         \State \hspace{63pt}  $ (1 - p_{query}[index]) $ 		        
		        \State $ index \gets index + 1 $
	        \EndWhile
	    \EndIf
		\EndFor
		\State \Return $likelihood$
		\EndFunction		
	\end{algorithmic}
\end{algorithm}

Hinsichtlich der Laufzeit benötigt das Hinzufügen einer Kante nach Angaben der graph-tool Dokumentation ~\cite{docs_graph_tool} eine Laufzeit von $ O(1) $. Da aber die Query- und die Referenzreads den bereits zuvor angelegten Knoten zugeordnet werden müssen, erfordert dies eine Suche der betreffenden Knoten. Dabei durchsucht graph-tool mit seiner Funktion \lstinline|find_vertex()| allein die Knoten und prüft auf die gesuchte Read-ID aus den FASTQ-Daten. Die ein- und ausgehenden Kanten der Knoten werden nicht beachtet, so dass eine Tiefen- oder Breitensuche des Graphen nicht notwendig ist und die Suche in $ O(V) $ durchgeführt werden kann ~\cite{graph_tool_coplexity_find_vertex}. Die Zuweisung der Kanteneigenschaften erfolgt jeweils in $ O(1) $, da diese direkt bei der Erzeugung der Kante hinzugefügt werden und keine vorherige Suche der Kante erforderlich ist. Für die Berechnung der Likelihood wird die geschätzte Fehlerrate $ p_{query} $ jeder Base verwendet, so dass die Anzahl der Berechnungen für jede Kante der Länge der Readsequenz $ k $ entspricht. Die Laufzeit für das Hinzufügen einer Kante beträgt somit $ O(k) $. Für alle Kanten ergibt sich daraus eine Gesamtlaufzeit von $ O(E\, \cdotp (k + V)) $. Bei realen Datensätzen gilt in der Regel $ k << V $ und die Länge der Reads variiert nur in einem engen Bereich, so dass $ k $ als vernachlässigbar klein und als nahezu konstant betrachtet werden kann. Dann ergibt sich aus Formel \eqref{eqn:3-7} eine Laufzeit von $O(E\, \cdotp V) $. 
\begin{equation} \label{eqn:3-7}
\tag{3-7}
 O(E\, \cdotp (k + V)) = O(E\, \cdotp V)
\end{equation}

Sind bei kleinen Datensätzen nur wenige Reads vorhanden, so dass $ k \leq V $, dann ergibt sich unter zusätzlicher Berücksichtigung von $k$ im Worst Case mit $ k = V $ nach Formel \eqref{eqn:3-8} ebenfalls eine Laufzeit von $O(E\, \cdotp V) $.
\begin{equation} \label{eqn:3-8}
\tag{3-8}
O(E\, \cdotp (k + V)) = O(E\, \cdotp (V + V)) = O(E\, \cdotp 2 \, \cdotp V) = O(E\, \cdotp V)
\end{equation}

Unter der Annahme, dass in seltenen Fällen die Readlänge, die meist nur wenige hundert Basenpaare zählt, tatsächlich die Anzahl der Reads übersteigt und somit $ k > V $ gilt, dann dominiert $k$ die Laufzeit. In diesem Fall wäre im Worst Case $k$ eine obere Schranke für $V$, so dass gilt:
\begin{equation} \label{eqn:3-9}
\tag{3-9}
O(E\, \cdotp (k + V)) = O(E\, \cdotp (k + k)) = O(E\, \cdotp 2 \, \cdotp k) = O(E\, \cdotp k)
\end{equation}

Da die Länge der Reads durch die gewählten Restriktionsenzyme sowie durch das Sequenzierverfahren selbst beschränkt ist, müsste ein solcher Datensatz relativ klein sein, so dass die damit verbundene Laufzeiterhöhung nur geringfügige Auswirkungen hätte. \\

Zusammenfassend soll daher für die folgenden Berechnungen vereinfachend der Mittelwert der Readlänge als konstant betrachtet werden, also $\overline{k}=const$, so dass für die Berechnung der Likelihoods zwischen den Reads eine Laufzeit von $O(\overline{k}) = O(1)$ veranschlagt wird. Für das Hinzufügen der Kanten des Graphen wird somit die Gesamtlaufzeit auf $ O(E\, \cdotp V) $ geschätzt.\\

Nach Abschluss der Graphkonstruktion werden für jedes Individuum noch einige Statistiken in die Log-Dateien geschrieben. Hier werden neben der Anzahl der Knoten und Kanten des Graphen auch die Anzahl der Substitutionen bzw. SNPs, Insertionen und Deletionen festgehalten, die beim Auslesen der CIGAR-Tupel registriert wurden. Zudem findet sich hier auch die maximal vorkommenden Edit-Distanz über alle Knoten, sofern sich diese unterhalb des festgelegten Schwellenwertes liegt. Ansonsten entspricht sie dem in der Konfigurationsdatei angegebenen Schwellenwert.\\

Als optionaler Output können über die Konfigurationsdatei und die Snakemake-Regel \lstinline|rule noderad| auch die detaillierten Graphinformationen sowie eine Visualisierung des Graphen ausgegeben werden. Die Graphinformationen wie Knoten, Kanten und ihre Eigenschaften können dabei im GraphMl-, DOT-, GML- oder im binären gt-Format gespeichert werden. Die graphische Darstellung wird als pdf-Datei ausgegeben, dabei entspricht die Kantenfärbung der berechneten Likelihood zwischen den Reads. \\

\subsection{Bestimmung der Zusammenhangskomponenten} \label{subsec:comp}

Die Bestimmung und Indexierung der Zusammenhangskomponenten erfolgt durch graph-tool selbst und kann in $ O(V + E) $ durchgeführt werden ~\cite{docs_graph_tool}. Die Indexnummer jeder Zusammenhangskomponente wird den in ihr enthaltenen Knoten als Knoteneigenschaft hinzugefügt. Zusammenhangskomponenten mit mehr als einem Knoten  werden als neuer eigenständiger Graph initialisiert und in einer Liste abgelegt. Hierfür wird aus dem Graphen für jede Komponente eine gefilterte Sicht erzeugt, die als neues Graph-Object gespeichert wird. Der Filtervorgang jeder Zusammenhangskomponente $ C $ muss für alle Knoten des Graphen durchgeführt werden, daher beträgt die Laufzeit hierfür $ O(C \, \cdotp V) $. Da alle weiteren Schritte des Algorithmus jeweils auf den einzelnen Komponenten durchgeführt werden, kann durch die Verwendung einer Liste von Graphen im Folgenden eine einfachen Iteration über die Komponenten in $ O(C) $ ausgeführt werden, ohne dass der Filtervorgang über alle Knoten jeder Komponente wiederholt werden muss. Zudem ermöglicht diese Datenstruktur eine effizientere Traversierung und Suche innerhalb der Zusammenhangskomponente, ohne dass für jede Komponente der gesamte Graph betrachtet werden muss. Der ursprüngliche Graph wird anschließend entfernt, um Arbeitsspeicher freizugeben. Auch dies erfolgt in konstanter Zeit. Die Laufzeit für die Extraktion der Zusammenhangskomponenten wird also bestimmt durch Identifikation, Indexierung und Filterung der Komponenten mit $ O(C \, \cdotp V) + O(V + E) = O(V \, \cdotp (C + 1) +E)$. Bei realen Daten gibt es in der Regel deutlich mehr Knoten als Cluster bzw. Zusammenhangskomponenten, so dass gilt $ C < V $. Würde im Worst Case aber jede Zusammenhangskomponente aus nur einem Knoten bestehen, also $ C = V $, so kann die maximale Laufzeit auf $ O(V \, \cdotp (C + 1) +E) = O(V \, \cdotp (V + 1) + E) = O(V^2 + E) $ geschätzt werden kann.\\

In der Log-Datei wird die Anzahl der Knoten aller Zusammenhangskomponenten als Histogramm festgehalten. Ebenso wird dort für alle Komponenten mit mehr als einem Element die Anzahl ihrer Knoten, Kanten und Eigenschaften aufgelistet.\\

Über die Konfigurationsdatei und die Snakemake-Regel \lstinline|rule noderad| können optional auch für die Zusammenhangskomponenten jeweils Visualisierungen und detaillierte Graphinformationen in den oben genannten Formaten (Kap. \ref{subsec:edges}) ausgegeben werden. Zudem kann optional auch der gesamte Graph mit den Komponentenindizes als Knoteneigenschaften gespeichert werden. In der visuellen Darstellung werden seine Knoten entsprechend der zugehörigen Zusammenhangskomponente eingefärbt, seine Kantenfärbung richtet sich weiterhin nach der aus \eqref{eqn:3-3} resultierenden Likelihood.

\subsection{Laufzeitanalyse zur Konstruktion des Graphen} \label{subsec:runtime_graph}
Wie an entsprechender Stelle bereits beschrieben, ist für die Erzeugung der Knoten eine Laufzeit von $ O(V^2) $ (Kap. \ref{subsec:nodes}) erforderlich, das Hinzufügen der Kanten kann in $ O(E\, \cdotp V) $ erfolgen. Somit wird für den vollständigen Aufbau des Graphen nach \eqref{eqn:3-10} eine Laufzeit von $ O(V \, \cdotp (V+E)) $ benötigt. \\
\begin{equation} \label{eqn:3-10}
\tag{3-10}
O(V^2) + O(E\, \cdotp V) = O(V \, \cdotp (V+E))
\end{equation}
In Zusammenschau mit der für die Extraktion der Zusammenhangskomponenten erforderliche Laufzeit von $ O(V^2 + E) $ (Kap. \ref{subsec:comp}) ergibt sich daraus nach  \eqref{eqn:3-11} eine Laufzeit von $ O(V \, \cdotp (V + E)) $. 

\begin{equation} \label{eqn:3-11}
\tag{3-11}
\begin{aligned}
&\ {} O(V \, \cdotp (V+E)) +O(V^2 + E) \\
& \ = O(V^2 + E \, \cdotp V + V^2 + E)\\
&\ = O(2 \, \cdotp V^2 + E \, \cdotp (V + 1)) \\
&\ = O(V^2 + E \, \cdotp V)\\
&\ = O(V \, \cdotp (V + E))\\
\end{aligned}
\end{equation}

\section{Bestimmung der maximalen Likelihood der Allele} \label{sec:max_lh_allele}
\subsection{Bestimmung der Allele und ihrer möglichen Häufigkeitsverteilung} \label{subsec:cand_allele}

Die einzelnen Zusammenhangskomponenten repräsentieren einen oder mehrere Loci. Alle weiteren Berechnungen aus diesem und den folgenden Kapiteln ~\ref{sec:max_lh_loci} und ~\ref{sec:vcf} werden für jede Komponente einzeln durchgeführt, dabei sollen die wahrscheinlichsten Allelsequenzen und die dazugehörigen Loci zu identifiziert werden.\\

Zunächst werden mit der Funktion \lstinline|get_candidate_alleles()| aus dem Modul \linebreak \lstinline|likelihood_operations.py| die Allelsequenzen ermittelt, die in der Zusammenhangskomponenten vorkommen. Für deterministischere Ergebnisse werden diese lexikographisch sortiert in Form einer Liste durch die Funktion zurückgegeben. Übersteigt die Größe des Clusters, d.h. die Knotenanzahl einer Zusammenhangskomponenten einen in der Konfigurationsdatei als \lstinline|treshold-cluster-size| festgelegten Wert, so wird von den im Cluster vorkommenden Sequenzen zunächst die absolute Häufigkeit bestimmt. Es werden dann nur diejenigen Sequenzen lexikographisch sortiert zurückgegeben, die einen weiteren, ebenfalls in der Konfigurationsdatei festgelegten Schwellenwert, \lstinline|treshold-seq-noise|, überschreiten. Dies dient dazu, bei großen Clustern Komplexität und Rechenaufwand zu reduzieren, da davon auszugehen ist, dass innerhalb eines großen Clusters echte Varianten einer Sequenz mehrfach vorkommen, wohingegen Artefakte und Sequenzierfehler eher vereinzelt auftreten. Dieses sog. Rauschen kann durch Anpassung der Schwellwerte in der Konfigurationsdatei herausgefiltert werden. Da bei kleineren Clustern auch einmalig registrierte Varianten einer Sequenz von Bedeutung sein können, soll der Filtervorgang erst ab einer festlegbaren Clustergröße durchgeführt werden. Für das Erstellen der Liste muss jeder Knoten in jeder Komponente betrachtet werden, so dass der Vorgang für alle Knoten in $ O(V) $ durchführbar ist.\\

Aus der so erzeugten Liste lexikographisch sortierter Kandidatenallele $ A_{observed} $ der Länge $ n_{observed} $ sollen nun die möglichen Häufigkeitsverteilungen in Abhängigkeit von der Ploidie bestimmt werden. Hierzu muss zunächst die aufgrund der Ploidie tatsächlich zu erwartende Anzahl von Allelen $n_{alleles}$ bestimmt werden. Die geschieht durch die Funktion \lstinline|get_max_parsimony_n_alleles()| in \lstinline|likelihood_operations.py|. Dadurch werden unnötige bzw. nicht mögliche Allelkombinationen eingespart und in der weiteren Berechnung nicht berücksichtigt. Ist die Ploidie $ \phi $ höher als die Anzahl der beobachteten Allele, so muss es mindestens genauso viele und bei Homozygotie auch mehrfach vorkommende Allele geben, damit die Ploidie erfüllt werden kann. Es muss also gelten $ n_{alleles} = \phi $. Wurden dagegen mehr Allele beobachtet als aufgrund der Ploidie möglich sind und die Ploidie ist Teiler von $n_{observed}$, so können alle beobachteten Allele auch tatsächlich vorkommen, da die Zusammenhangskomponente auch mehrere Loci enthalten kann. Es gilt dann also $ n_{alleles} = n_{observed} $. Ist dagegen die Anzahl der beobachteten Allele höher als die Ploidie, aber nicht ganzzahlig durch die Ploidie teilbar, so muss die Anzahl der Allele entsprechend erhöht werden. Das heißt, es müssen tatsächlich so viele Allele vorkommen, dass eine korrekte Ploidie erreicht wird, dass also die Ploidie eine Teiler von $ n_{alleles} $ wird. Die Anzahl der Allele muss also um die Ploidie abzüglich des Restes aus der Restdivision erhöht werden: $ n_{alleles} = n_{observed} + \phi - (n_{observed} \mod \phi)$. Diese Anpassung der Anzahl der Allele kann für jede Komponente $C$ in $ O(1) $ erfolgen, so dass die Laufzeit für alle Komponenten O(C) beträgt. Im Worst Case, bei dem jede Komponente nur einen Knoten enthält und somit $ C = V $ gilt, würde die Laufzeit maximal $ O(V) $ betragen.\\

Mit Hilfe der tatsächlich zu erwarteten Anzahl von Allelen $ n_{alleles} $ können nun alle Kombinationen möglicher Häufigkeitsverteilungen der Allele bestimmt werden (Funktion \linebreak \lstinline|get_candidate_vafs()| im Modul \lstinline|likelihood_operations.py|). Hierfür werden zunächst alle möglichen Allelkombinationen ermittelt. Dies erfolgt nach dem Urnenmodell unter Auswahl von $ n_{alleles} $ Elementen mit Zurücklegen aus insgesamt $ n_{observed} $ verschiedenen Elementen und ohne Berücksichtigung der Reihenfolge. Dadurch berechnet sich die Anzahl möglicher Kombination nach \eqref{eqn:3-12} aus dem  Binomialkoeffizienten der $k$-ten Ordnung aus $ n $ Elementen mit Zurücklegen ~\cite{tb_stat,bronst}.
\begin{equation} \label{eqn:3-12}
\tag{3-12}
\binom{n + k - 1}{k} = \frac{(n+k-1)!}{(n-1)!\, \cdotp k!} = \frac{(n_{alleles}+n_{observed}-1)!}{(n_{alleles}-1)!\, \cdotp n_{observed}!} 
\end{equation}

Diese Allelkombinationen werden mit Hilfe der Funktion  \lstinline|combinations_with_replacement| aus der Python-Library \lstinline|itertools| erzeugt \cite{itertools}.\\
\\
\definecolor{light-gray}{gray}{0.93}
\fcolorbox{black}{light-gray}{
	\parbox{\textwidth}{
		\vspace{0.5cm}
		\textbf{Beispiele möglicher Allelkombinationen:} \\	
		\\	
		$ ploidy = 2, n_{observed} = 2, n_{alleles} = 2$: \\
		{[(0, 0), (0, 1), (1, 1)]}\\
		\\
		$ ploidy = 2, n_{observed} = 3, n_{alleles} = 4$: \\
		{[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 2, 2), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 2, 2), (0, 2, 2, 2), (1, 1, 1, 1), (1, 1, 1, 2), (1, 1, 2, 2), (1, 2, 2, 2), (2, 2, 2, 2)]}
		\\
	}}\\


Für jedes Allel $i$ können im Anschluss aus seinen absoluten Häufigkeiten $H_{a_{i}}$ innerhalb jeder Allelkombination die relativen Häufigkeiten $h_{a_{i}}$ nach \eqref{eqn:3-13} bestimmt werden. 
\begin{equation} \label{eqn:3-13}
\tag{3-13}
h_{a_{i}} = \frac{H_{a_{i}}} {n_{alleles}}
\end{equation}
\\		
\fcolorbox{black}{light-gray}{
	\parbox{\textwidth}{
		\vspace{0.5cm}
		\textbf{Beispiele möglicher Häufigkeitsverteilungen der Allele:} \\	
		\\	
		$ ploidy = 2, n_{observed} = 2, n_{alleles} = 2$: \\
		{[1.0, 0.0]}, {[0.5, 0.5]}, {[0.0, 1.0]} \\
		\\
		$ ploidy = 2, n_{observed} = 3, n_{alleles} = 4$: \\
		{[1.0, 0.0, 0.0]}, {[0.75, 0.25, 0.0]}, {[0.75, 0.0, 0.25]}, {[0.5, 0.5, 0.0]}, {[0.5, 0.25, 0.25]}, {[0.5, 0.0, 0.5]}, {[0.25, 0.75, 0.0]}, {[0.25, 0.5, 0.25]}, {[0.25, 0.25, 0.5]}, {[0.25, 0.0, 0.75]}, {[0.0, 1.0, 0.0]}, {[0.0, 0.75, 0.25]}, {[0.0, 0.5, 0.5]}, {[0.0, 0.25, 0.75]}, {[0.0, 0.0, 1.0]}
		\\
}}
\linebreak 
\subsubsection{Abschätzung der oberenen Laufzeitschranke bei Kombinationen mit Wiederholung und Laufzeit der Funktion \lstinline|get_candidate_vafs()|} \label{subsubsec:cand_vafs}
Kombinationen mit Wiederholung werden sowohl für die Bestimmung der Allelkombinationen aus diesem Kapitel als auch später in Kap. \ref{subsec:comb_loci} für die Ermittlung möglicher Kombinationen von Loci genutzt. Hinsichtlich der Laufzeit und des Speicherplatzbedarfs kann diese Berechnung bei sehr großen Clustern zum dominierenden Faktor werden. Daher soll die Laufzeit an dieser Stelle genauer abgeschätzt werden. Seien dabei für eine bessere Übersichtlichkeit $ n_{alleles} $ als $ n $ und $ n_{observed} $ als $k$ bezeichnet und es gilt $n, k \in \mathds{N} $. Nach Formel \eqref{eqn:3-12} werden also $ \binom{n + k - 1}{k} $ Kombinationen jeweils in $O(1)$ erzeugt, d.h. in $ O(\binom{n + k - 1}{k}) $ für jede Zusammenhangskomponente. \\

Die Formel \eqref{eqn:3-12} ergibt sich aus der allgemeinen Formel des Binomialkoeffizienten \eqref{eqn:3-13}, durch den die Anzahl aller Kombinationen ohne Zurücklegen berechnet wird \cite{tb_stat}. 
\begin{equation} \label{eqn:3-13}
\tag{3-13}
\binom{n}{k} = \frac{n!}{(n-k)!\, \cdotp k!}
\end{equation} 
Mit anderen Worten, die Formel \eqref{eqn:3-12} kann auf zwei Arten interpretiert werden: es werden aus n Elementen $ k $ Elemente mit Zurücklegen gezogen oder es werden aus $ n + k - 1 $ Elementen $ k $ Elemente ohne Zurücklegen gezogen. Sei also  $ m = n + k - 1 $, dann gilt:
\begin{equation} \label{eqn:3-14}
\tag{3-14}
\binom{m}{k} = \frac{m!}{(m - k)!\, \cdotp k!} \leq m!
\end{equation} 

Daraus ergibt sich eine Laufzeit zunächst eine obere Schranke der Laufzeit von $ O(m!) = O((n + k)!) $. Hierfür soll nun eine kleinere obere Schranke gefunden werden. Um die Fakultät näherungsweise zu berechnen kann die Stirlingsche Formel \cite{bronst} verwendet werden:
\begin{equation} \label{eqn:3-15}
\tag{3-15}
n! \approx \left( \frac{n}{e} \right) ^n \, \cdotp \sqrt{2 \, \cdotp \pi \, \cdotp n}
\end{equation} 

Somit gilt auch \eqref{eqn:3-16} und ist eine untere Schranke der Fakultät \cite{script_binom}.
\begin{equation} \label{eqn:3-16}
\tag{3-16}
n! \geq \left( \frac{n}{e} \right) ^n 
\end{equation} 

Durch \eqref{eqn:3-12} und \eqref{eqn:3-16} lässt sich durch einige Umformungen die obere Schranke der Laufzeit von ursprünglich $ O(n!) $  auf $ O\left( \left( \frac{e \, \cdotp (n + k - 1)}{k}\right)^k\right) $ eingrenzen:
\begin{equation} \label{eqn:3-17}
\tag{3-17}
\begin{aligned}
\binom{n + k - 1}{k} &\ {} \overset{\eqref{eqn:3-12}}{=}  \frac{(n+k-1)!}{(n-1)!\, \cdotp k!} = \frac{\prod\limits_{i=1}^{n+k-1}i}{\prod\limits_{i=1}^{n-1}i \; \cdotp \, \prod\limits_{i=1}^{k}i}  \\
&\ = \frac{\prod\limits_{i=n}^{n+k-1}i \; \cdotp \, \prod\limits_{i=1}^{n-1}i}{\prod\limits_{i=1}^{n-1}i \; \cdotp \, \prod\limits_{i=1}^{k}i} = \frac{\prod\limits_{i=n}^{n+k-1}i}{\prod\limits_{i=1}^{k}i}\\
&\ = \frac{(n + k - 1) \, \cdotp (n + k - 2)\, \cdotp \; \dots \; \, \cdotp n}{k!} \\
&\ \leq \frac{(n + k - 1)^k}{k!} \\
&\ \overset{\eqref{eqn:3-16}}{\leq} \frac{(n + k - 1)^k}{\left( \frac{k}{e} \right) ^k} \\
&\ = \left( \frac{e \, \cdotp (n + k - 1)}{k}\right)^k  \\
\end{aligned}
\end{equation} 
Der Binomialkoeffizient verfügt über bestimmte Eigenschaften, insbesondere gelten der Symmetrie- \eqref{eqn:3-18} und der Additionssatz \eqref{eqn:3-19} , die sich auch in der Struktur des Pascalschen Dreiecks widerspiegeln \cite{bronst}. 
\begin{equation} \label{eqn:3-18}
\tag{3-18}
\binom{n}{k} = \binom{n}{n - k}
\end{equation} 
\begin{equation} \label{eqn:3-19}
\tag{3-19}
\binom{n}{k} + \binom{n}{k + 1} = \binom{n + 1}{k + 1} 
\end{equation} 
Aufgrund der Symmetrie sind die Binomialkoeffizienten jeder Zeile des Dreiecks $\binom{n}{k_{i}}$ mit $ k_{i} \in 1, \dots n $ und $ i \in \mathds{N} $ symmetrisch im Bezug auf die Zeilenmitte. Und da nach \eqref{eqn:3-19} jeder Koeffizient aus der Summe der beiden darüber liegenden Koeffizienten entsteht, sind die Werte der Koeffizienten zu den Zeilenrändern hin abnehmend und weisen in der Zeilenmitte ihr Maximum auf. Ist $n$ gerade, so befindet sich die Zeilenmitte bei $ k = \frac{n}{2} $. Ist $n$ ungerade so befinden sich die Maxima bei $ k = \lceil \frac{n}{2} \rceil $ und $ k = \lfloor \frac{n}{2} \rfloor $ und besitzen aufgrund der Symmetrie den gleichen Wert. Vereinfacht gilt also für ein gegebenes $n$, dass der Binomialkoeffizient $\binom{n}{k_{i}}$ bei $ k = \frac{n}{2} $ maximal ist. \\

Daher soll $ k = \frac{n}{2} $ als Worst Case angenommen werden, dann ergibt sich aus \eqref{eqn:3-17} eine Laufzeit von $ O(3e^{\frac{n}{2}} ) \in O(e^n) $. 
\begin{equation} \label{eqn:3-20}
\tag{3-20}
\begin{aligned}
\left( \frac{e \, \cdotp (n + k - 1)}{k}\right)^k &\ {} = \left( \frac{e \, \cdotp (n + \frac{n}{2} - 1)}{\frac{n}{2}}\right)^{\frac{n}{2}}  \\
&\ = \left( \frac{\frac{3en - 2e}{2}} {\frac{n}{2}}\right)^{\frac{n}{2}} \\
&\ = \left( \frac{3en - 2e}{n}\right)^{\frac{n}{2}} \\
&\ \leq \left( \frac{3en}{n}\right)^{\frac{n}{2}}\\
&\ = 3e^{\frac{n}{2}}\\
\end{aligned}
\end{equation}
Hinsichtlich der Allelkombinationen entspricht dies einer Zeit von $ O(e^{n_{alleles}}) $ für jede Zusammenhangskomponente. Für die Laufzeit über alle Komponenten $C_{i}$ gilt dann \linebreak $O\left( \sum\limits_{i=1}^{|C|}e^{n_{alleles}}\right) $, es dominiert also die Komponente, welche die meisten Knoten enthält. \\

Für sehr große Cluster ist daher die Laufzeit problematisch. Ebenso kommt es zu einem hohen Bedarf an Arbeitsspeicher, aufgrund der großen Anzahl von Kombinationen die dabei erzeugt werden. Um dies bedingt kompensieren zu können, wurden die oben beschriebenen Schwellenwerte eingeführt, die ab einer festgelegten Clustergröße \lstinline|treshold-cluster-size| nur Allelkandidaten ab einer bestimmten Häufigkeit ihrere Sequenz in den Reads \linebreak \lstinline|treshold-seq-noise| berücksichtigen.\\

Die Berechnung der Häufigkeitsverteilungen erfordert weitere Iterationen über die ermittelten Allelkombinationen, dabei werden sämtliche Allele für jede Kombination betrachtet. Dadurch erhöht sich die Laufzeit auf $ O(e^{n_{alleles}})\, \cdotp O(n_{alleles}) \, \cdotp O(n_{observed}) $. Den höchsten Wert nimmt $ n_{alleles} $ in dem Fall an, dass die Restdivision $ n_{observed} \mod \phi = 1 $, denn dann gilt  $ n_{alleles} = n_{observed} + \phi - 1 $. Da $\phi$ mit für alle Zusammenhangskomponenten den gleichen Wert besitzt, gilt $\phi = const$. Daraus ergibt sich nach \eqref{eqn:3-21} eine Laufzeit von $ O(e^{n_{observed}}\, \cdotp n_{observed}^2) $ für die Funktion  \lstinline|get_candidate_vafs()|.
\begin{equation} \label{eqn:3-21}
\tag{3-21}
\begin{aligned}
O(e^{n_{alleles}} \, \cdotp n_{alleles} \, \cdotp n_{observed}) 
&\ {}= O(e^{n_{observed} + \phi - 1} \, \cdotp (n_{observed} + \phi - 1) \, \cdotp n_{observed})\\
&\ = O(e^{n_{observed} + \phi}\, \cdotp n_{observed}^2) \, \cdotp \phi\\
&\ = O(e^{n_{observed}}\, \cdotp n_{observed}^2)\\
\end{aligned}
\end{equation} 

\subsection{Berechnung der Likelihoods der Allele anhand der möglichen Häufigkeitsverteilungen} \label{subsec:lh_allele}

Nun soll nach \eqref{eqn:2-xxx1} die Wahrscheinlichkeit ermittelt werden, einen Read anhand der gegebenen Allelfraktion zu beobachten (vgl. Kap. \ref{subsec:sol_allele_lh}). Dies geschieht in der Funktion \lstinline|get_allele_likelihood_read()| aus dem Modul \lstinline|likelihood_operations.py|). Für jeden Read wird dabei zunächst die Wahrscheinlichkeit bestimmt, dass der Read durch Sequenzierfehler aus einem der Kandidatenallele entstanden ist. Hierzu wird für jeden Read $ r_{i} $ jeweils eine ausgehende Kante zu jedem Kandidatenallel $ a_{j} $ gesucht und die nach \eqref{eqn:3-2} berechnete Likelihood aus den Eigenschaften dieser Kante zurückgegeben.\\

Geordnet nach dem Laufzeitaufwand werden für das Auffinden einer solchen Kante verschiedene Möglichkeiten geprüft:
\begin{enumerate}
	\item \label{-1-} Der Read besitzt eine ausgehende Kante zu einem Knoten, der mit der Sequenz des Kandidatenallels übereinstimmt. Hierfür werden alle ausgehenden Nachbarn des Knotens hinsichtlich ihrer in den Knoteneigenschaften abgelegten Sequenz geprüft. Wurde eine solche Kante gefunden, so wird die Likelihood der Kante direkt zurückgegeben. Andernfalls erfolgt die Suche wie unter beschrieben. 
	\item \label{-2-} Existiert keine Kante vom Knoten des Reads zu einem Knoten mit der Kandidatenallelsequenz, so wird nach einer entgegengesetzt gerichteten Kante gesucht. Dazu werden alle eingehenden Nachbarn auf die Sequenz des Kandidatenalles überprüft. Falls es eine solche Kante gibt, dann kann die gesuchte Likelihood der Gegenrichtung, über die Funktion \lstinline|get_alignment_likelihood()| mit dem Argument \lstinline|reverese = True| bestimmt werden.
	\item \label{-3-} Falls auch in Gegenrichtung keine passende Kante gefunden wurde, so wird in der gesamten Zusammenhangskomponente nach einer Kante gesucht, die zwei Knoten verbindet, deren Sequenzen denen von $ r_{i} $ und $ a_{j} $ entsprechen. Existiert eine solche Kante, so werden ihre CIGAR-Tupel verwendet, um für $ r_{i} $ die Likelihood zu bestimmen. Da die CIGAR-Tupel allein über die Sequenz definiert werden, gelten die so ermittelten CIGAR-Tupel aufgrund identischer Sequenzen der begrenzenden Knoten auch für eine nicht existente Kante zwischen $ r_{i} $ und $ a_{j} $. Die CIGAR-Tupel können also verwendet werden, um zusammen mit den nach  \eqref{eqn:3-1} berechneten p-Werten des Phred Quality Scores  von $ r_{i} $ die Likelihood mittels \lstinline|get_alignment_likelihood()| zu bestimmen.
	\item \label{-4-} Existiert auch im gesamten Graphen keine vergleichbare Kante, die $ r_{i} $ und $ a_{i} $ mit einander verbindet, so wird der Wert Null zurückgegeben.
\end{enumerate}

Zur Veranschaulichung ist der beschriebene Algorithmus zudem in Pseudocode dargestellt \ref{alg:r-a-lh}. Dabei sind $ C_{k} \in \{C_{1}, \dots ,C_{p}\} $ der Subgraph einer Zusammenhangskomponente, $ r_{i} \in C_{k} $ der Knoten des Reads und $ a_{j} \in A $ ein Kandidatenallel mit der Sequenz $ s_{a_{j}} $, wobei $ k,i,j \in \mathds{N} $. Die Verwendung fester Knoten- oder Kanteneigenschaften ist durch eckige Klammern gekennzeichnet, so ruft beispielsweise $r_{i}[p\_values]$ die $ p $-Werte des Phred Quality Scores von $r_{i}$ auf.\\

\renewcommand{\algorithmiccomment}[1]{\hfill$\triangleright$\textit{#1}}
\begin{algorithm}[H]
	\caption{Bestimmung der Likelihood zwischen einem Read und einem Kandidatenallel} \label{alg:r-a-lh}
	\begin{algorithmic}[1]	
		\Function{get\_allele\_likelihood\_read}{$ C_{k} $, $ r_{i} $, $ s_{a_{j}} $}
		\State $ out\_neighbors \gets get\_out\_neighbors(r_{i})$
		\For {$out\_neighbor \in out\_neighbors $}
		\If{$ s_{a_{j}} = out\_neighbor[sequence] $}
		\State \Return $ edge(r_{i}, out\_neighbor)[likelihood]$
		\EndIf
		\EndFor
		\State $ m\_{rates} \gets C_{k}[m_{sub},m_{ins},m_{del}] $
		\State $ qual \gets r_{i}[p\_values] $
		\State $ in\_neighbors \gets get\_in\_neighbors(r_{i})$		
		\For {$in\_neighbor \in in\_neighbors $}
		\If{$ s_{a_{j}} =  in\_neighbor[sequence] $}		
		\State $ cig \gets edge(in\_neighbor, r_{i})[CIGAR-Tuples]$		
		\State $ rev \gets True $
		\State \Return $get\_alignment\_likelihood(m\_{rates},\, cig,\, qual,\, rev)$   \algorithmiccomment{Alg. \ref{alg:lh_read}}
		\EndIf		
		\EndFor
		\State $ cigar \gets get\_cigar\_tuples(C_{k},\, r_{i}[sequence],\, s_{a_{j}}) $ \algorithmiccomment{Alg. \ref{alg:cig}}		
		\If{cigar exists}
		\State $ cig \gets cigar[0]$
		\State $ rev \gets cigar[1]$
		\State \Return $get\_alignment\_likelihood(m\_{rates},\, cig,\, qual,\, rev)$   \algorithmiccomment{Alg. \ref{alg:lh_read}}
		\EndIf
		\State \Return 0
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Im folgenden soll nun die Laufzeit der verschiedenen Suchvorgänge in \linebreak \lstinline|get_allele_likelihood_read()| genauer betrachtet werden. Dabei ist zu berücksichtigen, dass im Allgemeinen der Aufruf einer Kante bei bekannten Source- und Target-Knoten von ihren Knotengraden abhängt. Im Bezug auf die Knotengrade des Readknotens $d(r_{i}) $  und des Kandidatenallels $d(a_{j}) $ ist eine solche Suche durch graph-tool in $O(\min (d(r_{i}), d(a_{j})))$ durchführbar ~\cite{docs_graph_tool}. Im Worst Case sind alle Knoten einer Zusammenhangskomponente mit den beiden Knoten $r_{i}$ und $a_{j}$ verbunden, so dass diese jeweils einen Knotengrad besitzen, welcher der Anzahl der Knoten der Komponente $V_{c}$ mit Ausnahme des Knotens selbst entspricht. In diesem Fall beträgt die Laufzeit für den Aufruf einer Kante im Graphen $ O(V_{c}) $ \eqref{eqn:3-22}.
\begin{equation} \label{eqn:3-22}
\tag{3-22}
O(\min (d(r_{i}), d(a_{j}))) =O(\min (d(V_{c}+1), d(V_{c}-1)))= O(V_{c})
\end{equation} 

Im Hinblick auf \lstinline|get_allele_likelihood_read()| müssen im Worst Case alle Schritte (\ref{-1-}) bis (\ref{-4-}) ausgeführt werden, so dass sich ihre Laufzeiten summieren. In der unter (\ref{-1-}) beschriebenen Suche, werden alle ausgehenden Nachbarn des Reads betrachtet, ihre Anzahl entspricht also dem ausgehenden Knotengrad des Readknotens $d(r_{i_{out}})$. War die Suche erfolgreich, so muss die Likelihood der betreffenden Kante aus den Kanteneigenschaften ausgegeben werden. Hierfür ist ein Kantenaufruf wie oben beschrieben in $ O(V_{c}) $ notwendig. Im ungünstigsten Fall hat jeder Knoten nur ausgehende Kanten und zwar zu allen anderen Knoten der Zusammenhangskomponente, dann beträgt die Laufzeit nach \eqref{eqn:3-23} für alle Reads der Komponente $ O(V_{c}^2)$.
\begin{equation} \label{eqn:3-23}
\tag{3-23}
O(V_{c}) \, \cdotp (O(d(r_{i_{out}})) +  O(V_{c})) = O(V_{c} \, \cdotp ((V_{c}-1) + V_{c})  = O(V_{c}^2)
\end{equation} 

Die Suche bei (\ref{-2-}) erfolgt analog über alle eingehenden Nachbarn in $ O(V_{c}^2)$, allerdings mit zusätzlicher Likelihoodberechnung der rückläufigen Kante in $O(k)$. Daraus ergibt sich insgesamt eine Laufzeit von $ O(V_{c}^2 +k) = O(V_{c}^2)$, da $\overline{k}=const$ (siehe Kap. \ref{subsec:runtime_graph}).\\

Die Laufzeit für das Auffinden eine geeigneten CIGAR-Tupels (\ref{-3-}) kann in $O(V_{c}^3)$ erfolgen (siehe Formel \eqref{eqn:3-25}). Auch hier führt die Likelihoodberechnung einer einzelnen Kante zu keiner relevanten Änderung der Laufzeit.\\

Die Rückgabe eines konstanten Wertes bei (\ref{-4-}) benötigt lediglich eine Laufzeit von $ O(1) $.\\

Aus den insgesamt vier Schritten, die im ungünstigsten Fall alle ausgeführt werden ergibt sich für \lstinline|get_allele_likelihood_read()| eine Gesamtlaufzeit von $O(V_{c}^3)$. \\
\begin{equation} \label{eqn:3-24}
\tag{3-24}
\begin{aligned}
&\ {} O(V_{c}^2) + O(V_{c}^2) + O(V_{c}^3) + O(1))  \\
& \ = O(2 \, \cdotp V_{c}^2 + V_{c}^3 )\\
&\ = V_{c}^3\\
\end{aligned}
\end{equation}

Wie bereits erwähnt, kann die Bestimmung der Wahrscheinlichkeit, dass ein Read durch Sequenzierfehler aus einem bestimmten Allel entstanden ist, erfordern, die CIGAR-Tupel des Sequenzvergleichs zu identifizieren. Auch im Folgenden (Kap. \ref{sec:max_lh_loci}) kann die Zuordnung der Kandidatenallele zu bestimmten Loci im Hinblick auf die Heterozygotiewahrscheinlichkeiten, die Ermittlung der CIGAR-Tupel zwischen den Kandidatenallelen bedingen. Dies wird in NodeRAD durch die Methode \lstinline|get_cigar_tuples()| bewerkstelligt, die sich ebenfalls im Modul \lstinline|likelihood_operations.py| befindet. Sie benötigt als Eingabeparameter eine Graphen oder Subgraphen sowie zwei Sequenzen: die Query-Sequenz $ s_{source} $ und die Referenz-Sequenz $ s_{target} $. Der Graph wird nach einer Kante durchsucht, die Knoten miteinander verbindet, welche die beiden gegebenen Sequenzen besitzen. Dabei werden nicht nur Kanten berücksichtigt, die von der Query- zur Referenzsequenz verlaufen, sondern auch entgegen gerichtete Kanten, die von der Referenz- zur Querysequenz verlaufen. Zur Markierung der Kantenrichtung gibt die Funktion neben dem CIGAR-Tupel auch einen booleschen Wert zurück. Dieser wird in den Funktionen \lstinline|get_alignment_likelihood()| (Kap. \ref{subsec:edges}) und \lstinline|get_heterozygosity()| (Kap. \ref{subsec:lh_loci}) für die Likelihoodberechnung benötigt. Im Falle einer entgegen gerichteten Kante, also bei \lstinline|reverse=True|, werden Deletionen als Insertionen gewertet und umgekehrt. Bei erfolgreicher Sucher gibt die Funktion neben dem booleschen Wert auch die gefundenen CIGAR-Tupel zurück, ansonsten wird der Wert $ None $ zurückgegeben.\\

Um eine passende Kante im Graphen zu finden, erstellt \lstinline|get_cigar_tuples()| zunächst eine Liste aller Knoten $ R_{source} = (v_{1}, \dots, v_{i})$, welche die Query-Sequenz $ s_{source} $ besitzen sowie eine Liste aller Knoten $ R_{target} = (w_{1}, \dots, w_{j}) $, die die Referenz-Sequenz $ s_{target} $ tragen. Hierfür werden für jede Sequenz alle Knoten des Graphen mit \lstinline|find_vertex()| aus graph-tool durchsucht. Nun wird nach einer Kante gesucht, die einen der Knoten aus $ R_{source} $ mit einem der Knoten aus $ R_{target} $ verbindet. Es erfolgt also ein Kantenaufruf für jeden Knoten aus $ R_{source} $ in Kombination mit jedem  Knoten aus $ R_{target} $. Der Kantenaufruf erfolgt sowohl für die angegebene Kantenrichtung und falls erfolglos auch für die entgegengesetzte Richtung. Existiert eine solche Kante, dann werden ihre CIGAR-Tupel sowie der boolsche Wert entsprechend der Kantenrichtung zurückgegeben. \\

Zur Veranschaulichung ist der Algorithmus in \ref{alg:cig} zudem als Pseudocode dargestellt. Dabei sind $ C_{k} \in \{C_{1}, \dots ,C_{p}\} $ der Subgraph einer Zusammenhangskomponente, $ s_{query} $ die Query-Sequenz und $ s_{ref} $ die Referenz-Sequenz. Die Verwendung fester Knoten- oder Kanteneigenschaften ist durch eckige Klammern gekennzeichnet, so ruft beispielsweise $v_{i}[sequence]$ die Sequenz des Knotens $v_{i}$ aus den Knoteneigenschaften ab. \\

\begin{algorithm}[H]
	\caption{CIGAR-Tupel bestimmen}  \label{alg:cig}
	\begin{algorithmic}[1]	
		\Function{get\_cigar\_tuples}{$ C_{k} $, $ s_{query} $, $ s_{ref} $}
		\State $ R_{source} \gets \{v_{i} \in C_{k} \wedge i,k \in \mathds{N} \; |\; v_{i}[sequence]= s_{query} \}$
		\State $ R_{target} \gets \{w_{j} \in C_{k} \wedge j,k \in \mathds{N} \; |\; w_{j}[sequence]= s_{ref} \}$
		\For{$ v_{i} \in R_{source} $}
		    \For{$ w_{j} \in R_{target} $}
		        \If{$ edge(v_{i}, w_{j})$ exists}
		            \State \Return $ (\;edge(v_{i}, w_{j})[CIGAR-Tuples],\, False\;) $		    
		        \EndIf
		        \If{$ edge(w_{j}, v_{i})$ exists}
		            \State \Return $ (\;edge(w_{j}, v_{i})[CIGAR-Tuples],\, True\;) $		
		        \EndIf
		    \EndFor
		\EndFor
		\State \Return $ None $
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Die hierfür erforderliche Laufzeit setzt sich zusammen aus den Suchvorgengen für die beiden Knotenmengen $ R_{source} $ und $ R_{target} $, diese ist in graph-tool in $O(V_{c} + V_{c}) = O(V_{c})$ durchführbar (Kap. \ref{subsec:edges}). Seien im Worst Case die Hälfte aller Knoten der Zusammenhangskomponente in $ R_{source} $ und die andere Hälfte in $ R_{target} $. Um die Eigenschaft der Zusammenhangskomponente zu gewährleisten muss daher eine Kante zwischen beiden Knotenmenge existieren. Sei dies zudem eine entgegen gerichtete Kante. Sind die beiden Knoten $ (v, w) $ dieser Kante allerdings jeweils an letzter Position in $ R_{source} $ und $ R_{target} $, so erfolgen für alle übrigen Knoten der Komponente Kantenaufrufe für die Hin- und Rückrichtung. Wie bereits oben beschrieben, benötigt ein Kantenaufruf eine Laufzeit von $ O(\min (d(v), d(w)))$, mit den Knotengraden $d(v) $ von Knoten $v \in R_{source}$ und $ d(w) $ von Knoten $ w \in R_{target} $ ~\cite{docs_graph_tool}. Seien nun außerdem $ v $ mit allen Knoten in $ R_{source} $ und $ w $ mit allen Knoten in $ R_{target} $ verbunden, so besitzt $v$ einen Knotengrad von $ d(v) = \frac{V_{c}-1}{2} $. Der Knoten $ w $ besitzt einen Knotengrad von $ d(w) = \frac{V_{c}}{2} $, da eine entgegen gerichtete Kante von $w$ zu $v$ verläuft. Dann ergibt sich nach \eqref{eqn:3-25} eine Gesamtlaufzeit für \lstinline|get_cigar_tuples()| von $ O(V_{c}^3) $. \\

\begin{equation} \label{eqn:3-25}
\tag{3-25}
\begin{aligned}
&\ {} O(V_{c}) + O(V_{c}^2 \, \cdotp \min (d(v), d(w)))  \\
& \ = O\left( V_{c} + V_{c}^2 \, \cdotp \min \left( \frac{V_{c}-1}{2}\,, \frac{V_{c}}{2}\right) \right) \\
&\ = O(V_{c} + V_{c}^2 \, \cdotp V_{c}) \\
&\ = O(V_{c}^3) \\
\end{aligned}
\end{equation}

In  \lstinline|get_allele_likelihood_read()| wurden also die Wahrscheinlichkeiten jedes Reads bestimmt, dass er von den verschiedenen Allele der Zusammenhangskomponente stammt (siehe oben). Die Wahrscheinlichkeiten im Bezug auf jedes Allel sollen nun in die zuvor bestimmten Kombinationen der Häufigkeitsverteilungen der Allele (siehe Kap. \ref{subsec:cand_allele}) nach Formel \eqref{eqn:2-xxx2} einbezogen werden. In der Funktion \lstinline|calc_vafs_likelihood_read()| aus dem Modul \lstinline|likelihood_operations.py| wird also für jeden Read die Wahrscheinlichkeit berechnet, diesen unter der gegebenen Allelfraktion zu beobachten. Dazu wird über die relativen Häufigkeiten jedes Allels einer gegebenen VAFs-Kombination iteriert und ihr Produkt mit der Readlikelihood aufsummiert. Da die Läge der Allelfraktion genau der Anzahl der Allele einer Zusammenhangskomponenten $ n_{observed} $ entspricht, erhöht sich hierdurch die Laufzeit um den Faktor $ O(n_{observed}) $. \\

Aus dem Produkt über alle Reads wird anschließend für eine gegebene Allelfraktion nach Formel \eqref{eqn:2-xxx3} in \lstinline|calc_vafs_likelihood()| die Gesamtwahrscheinlichkeit errechnet. Dadurch wird dir Laufzeit zusätzlich um den Faktor der Anzahl der Reads der Zusammenhangskomponente $ O(V_{c}) $ erhöht.\\

Auf diese Weise wird in \lstinline|noderad_main.py| für jede Allelfraktion die Gesamtwahrscheinlichkeit über alle Reads und alle relativen Häufigkeiten der Allele berechnet und in einer Liste abgelegt. Dazu muss über alle VAFs iteriert werden. Die Anzahl der VAFs entspricht dabei genau der Anzahl der Allelkombinationen. Wie bereits in Kap. \ref{subsubsec:cand_vafs} besprochen, kann diese hinsichtlich der Laufzeit mit $ O(e^n_{alleles}) $ abgeschätzt werden (Formel \eqref{eqn:3-20}). Da im Worst Case $n_{alleles} = n_{observed} + \phi - 1$, ergibt sich eine Laufzeit von $ O(e^{n_{observed} + \phi - 1}) \in O(e^{n_{observed}}) $. Um diesen Faktor erhöht sich also die Gesamtlaufzeit bei der Berechnung der Liste der Gesamtwahrscheinlichkeiten. \\

Aus dieser Liste wird die Allelfraktion mit maximaler Likelihood als wahrscheinlichste Lösung gewählt und in Kap. \ref{sec:max_lh_loci} verwendet, um die wahrscheinlichsten Loci zu ermitteln, die diese Häufigkeitsverteilung der Allele erklären können. Hinsichtlich der Laufzeit muss jede Lösung der Liste betrachtet werden. Da die Länge der Liste der Anzahl der VAFs entspricht, kann die Laufzeit hierfür nach \eqref{eqn:3-20} auf maximal $ O(e^{n_{observed}}) $ geschätzt werden. \\

Als zusätzliche Statistiken werden in den Log-Dateien für jede Komponente die Anzahl der Allele, die Ploidie sowie die Häufigkeitsverteilung Allelfraktion mit der maximaler Likelihood und den dazugehörigen Allelsequenzen festgehalten.

\subsubsection{Gesamtlaufzeit für die Bestimmung der Allelfraktion mit maximaler Likelihood}
Die geschätzten Laufzeiten der einzelnen Schritte wurden bereits an entsprechender Stelle diskutiert und sollen hier noch einmal zusammenfassend aufgeführt werden. Zur übersichtlicheren Darstellung soll die Anzahl der Allele hier nur noch mit $n$ bezeichnet werden, dies entspricht der bisher verwendeten Variable $n_{observed}$. Für jede Zusammenhangskomponente werden vorab die Kandidatenallele bestimmt $ O(V_{c}) $, die Anzahl der tatsächlich zu erwartenden Allele berechnet $ O(V_{c}) $ und die Kombinationen möglicher Häufigkeitsverteilungen ermittelt $ O(e^{n} \, \cdotp n^2) $. Anschließend erfolgt die Likelihoodberechnung in $ O(V_{c}^3) $ über alle Allelfraktionen $ O(e^{n}) $, für alle Reads $ O(V_{c}) $ und im Vergleich jedes Reads zu allen Kandidatenallelen $ O(n) $. Aus den Lösungen wird im Anschluss das Maximum bestimmt $ O(e^{n}) $. Daraus ergibt sich nach \ref{eqn:3-26} eine Gesamtlaufzeit für die Bestimmung der Allelfraktion mit maximaler Likelihood von $ O(e^n \, \cdotp (n^2 + n \, \cdotp V_{c}^4)) $. 
\begin{equation} \label{eqn:3-26}
\tag{3-26}
\begin{aligned}
  O(V_{c}) +  O(V_{c}) &\ {} + O(e^n \, \cdotp n^2) + O(e^n) \, \cdotp O(V_{c}) \, \cdotp O(n) \, \cdotp O(V_{c}^3) + O(e^n)\\
&\ =  O(2 V_{c} + e^n \, \cdotp n^2 + e^n + e^n \, \cdotp n \, \cdotp V_{c}^4)\\
&\ = O(2 V_{c} + e^n \, \cdotp (n^2 + 1 + n \, \cdotp V_{c}^4))\\
&\ = O(e^n \, \cdotp (n^2 + n \, \cdotp V_{c}^4)) \\
\end{aligned}
\end{equation}

Im Worst Case entspricht $ n $ genau der Anzahl der Reads in der Zusammenhangskomponente und es gilt $ n = n_{observed} = V_{c} $, so dass die Laufzeit dann auf $ O(e^{V_{c}} \, \cdotp V_{c}^5) $ geschätzt werden kann \eqref{eqn:3-27}.
\begin{equation} \label{eqn:3-27}
\tag{3-27}
\begin{aligned}
O(e^n \, \cdotp (n^2 + n \, \cdotp V_{c}^4))
&\ {} = O(e^{V_{c}} \, \cdotp (V_{c}^2 + V_{c} \, \cdotp V_{c}^4))\\
&\ = O(e^{V_{c}} \, \cdotp (V_{c}^2 + V_{c}^5))\\
&\ = O(e^{V_{c}} \, \cdotp V_{c}^5) \\
\end{aligned}
\end{equation}

Die Laufzeit wird also durch die Anzahl der VAFs dominiert und kann bei großen Clustern mit vielen Allelen problematisch werden. Die konfigurierbaren Schwellenwerte \lstinline|treshold-cluster-size| und \lstinline|treshold-seq-noise| ermöglichen eine Anpassung solcher großen Cluster, um die Anzahl der Kombinationen und damit den Rechenaufwand flexibel reduzieren zu können.

\section{Bestimmung der maximalen Likelihood der Loci} \label{sec:max_lh_loci}

\noindent======================= draft =======================\\
Für jede Zusammenhangskomponente\\
jede zusammenhangskomponente kann auch mehr als nur einen Locus beinhalten.\\
\subsection{Bestimmung der möglichen Loci} \label{subsec:comb_loci}
ermittlung der möglichen loci-Kombinationen erfolgt in der Funktion get\_candidate\_loci() analog zur Funktion get\_candidate\_vafs() (vgl. Kap \ref{subsec:cand_allele}):
\begin{itemize}
	\item auch hier ist die bereits beschriebene anzahl der tatsächlich zu erwartenden allele $n_{alleles}$ aus get\_max\_parsimony\_n\_alleles() grundlage für die berechnung der möglichen loci-Kombinationen 
	\item anschließend auch hier durch combinations\_with\_replacement (python-library itertools) alle Kombinationen mit wiederholung für $n_{alleles} $ bestimmen -> mehrfaches vorkommen erlaubt, reihenfolge irrelevant 
	\item die daraus resultierenden Allel-Kombinationen werden denn aber mit Hilfe der Funktion grouper (aus den itertools recipes der python-library itertools) zu gruppen entsprechend der in der Konfigurationsdatei angegebenen ploidie zusammengefasst.
	\item dadurch entstehen verschiedene Kombinationen von möglichen loci als listen von Tupeln:\\
		Bsp.: \\
	    $ n_{alleles} = 2, ploidy = 2$: \\
	    {[(0, 0)]}, {[(0, 1)]}, {[(1, 1)]} \\
	\item falls aufgrund der erwarteten tatsächlichen anzahl der allele $n_{alleles} $ mehrere Loci möglich sind, resultieren Kombinationen von Loci:\\
	    $ n_{alleles} = 3, ploidy = 2$: \\
	    {[(0, 0), (0, 0)]}, {[(0, 0), (0, 1)]}, {[(0, 0), (0, 2)]}, {[(0, 0), (1, 1)]}, {[(0, 0), (1, 2)]}, {[(0, 0), (2, 2)]}, {[(0, 1), (1, 1)]}, {[(0, 1), (1, 2)]}, {[(0, 1), (2, 2)]}, {[(0, 2), (2, 2)]}, {[(1, 1), (1, 1)]}, {[(1, 1), (1, 2)]}, {[(1, 1), (2, 2)]}, {[(1, 2), (2, 2)]}, {[(2, 2), (2, 2)]} \\
	\item jedes Tupel der Länge der ploidie entspricht also einem möglichen Locus
\end{itemize}

\subsection{Zuordnung der wahrscheinlichsten Allelverteilung zu passenden Loci} \label{subsec:lh_loci}
Für jede mögliche Kombination der Loci  aus get\_candidate\_loci() wird zunächst durch die Indikatorfunktion indicator\_constrait() nach Formel \eqref{eqn:2-xxx4} geprüft, ob sich die Loci dieser Kombination der zuvor bestimmten vaf mit maximaler Likelihood, also der wahrscheinlichsten Allelverteilung, zuordnen lassen. Ist die Bedingung der Indikatorfunktion erfüllt, so wird die Heterozygotiewahrscheinlichkeit für die allele der loci dieser Kombination berechnet und zurückgegeben. Ist die Bedingung der Indikatorfunktion nicht erfüllt, so wird eine Likelihood von 0 zurückgegeben.

Trifft die Bedingung der Indikatorfunktion zu, so folgt die Berechnung der wahrscheinlichsten Zuordnung der Loci zu den Allelen aus den max. Likelihood vafs. Dafür werden der Loci-Kombination zunächst die dazugehörigen allelsequenzen zugeordnet. Die allel-nummer in einer Loci-Kombination entspricht dabei dem index der liste der allelsequenzen der kandidaten-allele aus get\_candidate\_alleles() (siehe Kap. \ref{subsec:cand_allele}).

Analog zur Likelihoodberechnung beim paarweisen Vergleich der Reads unter Berücksichtigung der Sequenzierfehlerwahrscheinlichkeit \ref{subsec:edges} erfolgt auch die Berechnung der Likelihood für eine bestimmte Loci-Kombination durch den paarweisen Vergleich der Allele im Sinne eines pair Hidden Markov Models $ pairHMM_{\eta}(a_{l_{j,1}}, a_{l_{j,2}}) $ (vgl. Kap. ~\ref{subsec:sol_phmm}). Allerdings erfolgt nun der Vergleich unter Berücksichtigung der Heterozygotiewahrscheinlichkeit nach Formel \eqref{eqn:2-xxx5}. 

Durch die Funktion get\_allel\_linkelihood\_allele() werden dabei die Allelsequenzen der Loci-Kombination paarweise miteinander verglichen. Hierfür wird für jedes Paar der CIGAR-String durch die in Kap. ~\ref{subsec:lh_allele} bereits beschriebene Funktion get\_cigar\_tuples() ermittelt.

Der CIGAR-String wird anschließend dafür benötigt aus den paarweisen Kombinationen der Allelsequenzen innerhalb eines Locus die Likelihood im Hinblick auf die Heterozygotiewahrscheinlichkeit zu berechnen. Analog zum Algorithmus ~\ref{alg:lh_read} wird in der Funktion  get\_heterozygosity() die Likelihood über die Heterozygotiewahrscheinlichkeiten aller Matches und Mismatches berechnet.

Dabei entspricht im Falle eines Mismatches die Likelihood der in der Konfigurationsdatei angegebenen Heterozygotiewahrscheinlichkeit $ \eta $ für die betreffende Mutationsart, also $ \eta_{sub} $ für Substitutionen , $ \eta_{ins} $ für Insertionen bzw. $ \eta_{del} $ Deletionen. Sei $i$ der Index der betreffenden Base innerhalb der betrachteten Allelsequenz und $ \eta_{rate} \in \{\,\eta_{sub},\, \eta_{ins},\, \eta_{del}\,\}$, dann berechnet sich bei einem Mismatch die Likelihood $L_{i}$ der Base nach Formel \eqref{eqn:3-xxx3}.
\begin{equation} \label{eqn:3-xxx3}
\tag{3-xxx3}
L_{i\,_{mismatch}} = \eta_{rate}
\end{equation}

Im Falle eines Matches senkt die Möglichkeit eines Mismatches die Likelihood der betreffenden Base entsprechend um die Summe der Heterozygotiewahrscheinlichkeiten der genannten Mutationsarten (Formel \eqref{eqn:3-xxx4}).
\begin{equation} \label{eqn:3-xxx4}
\tag{3-xxx4}
L_{i\,_{match}} = 1 - (\eta_{sub} + \eta_{ins} + \eta_{del})
\end{equation}

Die Likelihood $ Pr(T=a_{l_{j,2}} \, | \, S=a_{l_{j,1}}, \eta) $ des paarweisen Vergleichs der Allele $a_{l_{j,1}}$ und $a_{l_{j,2}}$ hinsichtlich der Zuordnung zu den Loci-Kombinationen errechnet sich schließlich aus dem Produkt der Wahrscheinlichkeiten der einzelnen Basen:
\eqref{eqn:3-xxx5}).
\begin{equation} \label{eqn:3-xxx5}
\tag{3-xxx5}
Pr(T=a_{l_{j,2}} \, | \, S=a_{l_{j,1}}, \eta) = pairHMM_{\eta}(a_{l_{j,1}}, a_{l_{j,2}}) = \prod_{i=1}^{k}L_{i}
\end{equation}


komponentenweise mit $C = V$ im Worst Case (Kap. \ref{subsec:comp}) -> $O(V)$

\section{Ausgabe der wahrscheinlichsten Loci als VCF-Datei} \label{sec:vcf}

Ausgabe im VCF-Format, tab-separierter body\\
Als Header der Spalten: CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMA, <Name der sample>\\
get\_sorted\_loci\_alleles(): lexikographisch sortierte Liste der Menge (d.h. ohne Duplikate) der Allelsequenzen der loci-Kombination mit max. Likelihood $A_{opt}$-> der erste Eintrag wird in die REF-Spalte des VCF-Files eingetragen, alle übrigen kommasepariert in die ALT-Spalte\\

Da die vollständige Sequenz der Allele angegeben wird ist immer POS=1
CHROM wird für die Bennennung der Loci genutzt im Format LOC<ldf.-Nr>
ID, QUAL, FILTER und INFO werden hier nicht genutzt und mit einem dot als Platzhalter bei allen Loci befüllt\\
FORMAT für alle Loci wird der Genotype-Datentyp ''GT'' angegeben, dieser wird in der Spalte der sample spezifiziert. Da die Indizes der Loci der gesamten Liste der Kandidaten-Allele $A_{cand}$ zugeordnet sind, muss ihre Indizierung nun auf $A_{opt}$ angepasst werden.  Dafür werden in get\_alleles\_matched\_to\_loci() zunächst jedem Locus der max Likelihood Loci-Kombination die dazugehörigen Allelsequenzen zugeordnet. Diese werden anschließend in get\_gt\_indices() den Indizes der entsprechenden Sequenz aus $A_{opt}$ zugeordnet, so dass das lexikographisch erste Allel, also REF mit 0 im Genotyp indiziert ist, die Allele aus ALT entsprechend mit höheren Indizes entsprechend ihrer Sortierung in $A_{opt}$. Nun lässt sich der Genotyp im Bezug auf die Sequenzen in REF und ALT aus diesen Indizes und getrennt durch Slashes direkt angeben. Dies geschieht in der Funktion get\_genotype(), deren Ergebnis dann in das sample-Feld eingetragen wird.\\
