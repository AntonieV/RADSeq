% kapitel3.tex
\chapter{Algorithmus} \label{sec:alg}
Für das hier implementierte RAD-Sequencing-Tool, NodeRAD, wurde zur Workflowintegration das Workflow Management System Snakemake verwendet ~\cite{koester_2012_1, koester_2012_2}. Die einzelnen Analyseschritte werden dabei über Regeln abgebildet. Für jede Regel können neben dem zu verwendenden Script oder Shell-Kommando sowie den Pfadangaben für In- und Output auch zusätzliche Optionen festgelegt werden. Dazu gehören beispielsweise Angaben zu Parametern bzw. Argumenten für die verwendeten Tools, Pfadangaben für Log-Dateien oder die Anzahl der zu verwendenden Threads.

Als Input benötigt der Workflow eine Datei im FASTQ-Format, welche die single-end Reads der verschiedenen Individuen mit ihren Identifikationsbezeichnungen, der Basensequenz und Angaben zur  Basenqualität enthält. Des Weiteren wird eine Tabelle im tsv-Format benötigt, in der die Zuordnung der Probennamen zu den Individuen und ihren Barcode-Sequenzen definiert ist. Nach dem Preprocessing, der Qualitätskontrolle der Reads und dem Sequence-Alignment erfolgt die RAD-Seq-Analyse durch NodeRAD. Hierbei werden die Wahrscheinlichkeiten der Allelsequenzen und der möglichen Loci bestimmt. Die Loci mit der höchsten Wahrscheinlichkeit werden schließlich mit den Sequenzen ihrer Allele und den möglichen Varianten entsprechend dem ermittelten Genotyp in einer Datei im Variant Call Format (VCF) ausgegeben.

\section{Preprocessing} \label{sec:preproc}

Im Preprocessing werden durch das Tool Cutadapt ~\cite{martin_2011} die Reads jedes Individuums anhand ihrer Barcodesequenzen identifiziert und extrahiert (Demultiplexing). Hiernach werden die Barcodesequenzen entfernt (Trimming) und die Reads jedes Individuums in separaten Dateien im FASTQ Format abgelegt. \\
Im Anschluss an das Trimming erfolgt eine Qualitätskontrolle durch das Tool FastQC  ~\cite{andrews_2012}. Dabei werden einige allgemeine Statistiken zu den Rohdaten der Reads generiert, wie beispielsweise zur Basenqualität, zum GC-Gehalt, dem Anteil an Duplikaten oder überrepräsentierten Sequenzen. Durch das Tool MultiQC ~\cite{ewels_2016} wird aus diesen Statistiken und den Log-Dateien von Cutadapt ein html-Report mit diversen Plots zur Veranschaulichung erstellt.

\section{Edit-Distanzen} \label{sec:edit}
Für die spätere Konstruktion eines Graphen basierend auf den Edit-Distanzen zwischen den Readsequenzen wird für jedes Individuum zunächst ein Sequenzalignment mit Hilfe des Tools Minimap2 ~\cite{li_2018} erstellt. Hierbei werden alle Readsquenzen paarweise verglichen und in Abhängigkeit von ihren Übereinstimmungen (Matches) und Unterschieden (Mismatches) einander zugeordnet. Das Ergebnis des Mappings wird im sam-Format ~\cite{li_2009} ausgegeben und enthält Angaben zur betrachteten Sequenz (Query), die gegen einen anderen Read (Reference) verglichen wurde. Neben den ID's der Query- und Reference-Sequenzen, werden dort unter anderem auch der CIGAR-String, Informationen zur Basenqualität der Query-Sequenz, sowie optional verschiedene Tags angegeben. Ein für die späteren Berechnungen wichtiges Maß sind die Edit-Distanzen, die durch den NM-Tag repräsentiert werden. Die Edit-Distanz gibt hierbei die minimale Anzahl von Editieroperationen an, um die Query-Sequenz in die Referenzsequenz zu transformieren. Als Editieroperationen sind hierbei ersetzen, einfügen und löschen von Basen möglich. Auf DNA-Ebene entspricht dies den Punktmutationen im Sinne von Substitutionen, Insertionen und Deletionen (vgl. Kap. \ref{subsec:mutation}). Der CIGAR-String ist eine kondensierte Darstellung der Unterschiede zwischen Query- und Reference-Sequenz. In ihm werden Matches und Mismatches wie Insertionen, Substitutionen und Deletion jeweils mit der Anzahl der betroffenen Basen angegeben. Sowohl der CIGAR-String als auch der NM-Tag definieren wichtige Kanteneigenschaften des späteren Graphen. \\

\section{Konstruktion des Graphen} \label{sec:graph}
\subsection{Knoten des Graphen} \label{subsec:nodes}
Das hier in Python implementierte Tool, NodeRAD, benötigt als Input zu jedem Individuum die getrimmten single-end Read-Daten sowie das Sequenzalignment. Zunächst wird daraus für jedes Individuum ein eigener, gerichteter Graph $ G $ mit $ G = (V,E) $ erstellt. Seine Knoten, $ V $, werden durch die einzelnen Reads repräsentiert. Entsprechend ergeben sich die Knoteneigenschaften aus den Daten der Reads, diese werden den FASTQ-Dateien nach Ausführung von Cutadapt (siehe \ref{sec:preproc}) entnommen. Die Kanten, $ E $, zwischen den Knoten ergeben sich aus dem Vergleich ihrer Sequenzen im Rahmen des Sequenzalignments mittels Minimap2 (siehe \ref{sec:edit}).\\

Zusätzlich entnimmt NodeRAD der Konfigurationsdatei des Workflows einige Konstanten und Grenzwerte für die späteren Berechnungen. Dazu gehören die Mutationsraten und Heterozygotiewahrscheinlichkeiten für Substitutionen, Insertionen und Deletionen, die Ploidie des Chromosomensatzes der untersuchten Spezies und Grenzwerte. Die Konstanten werden als Grapheigenschaften im Graphen abgelegt. Als konfigurierbare Grenzwerte gibt es für NodeRAD einen Schwellenwert für die maximal zulässige Editierdistanz, bei dem zwei Knoten noch durch eine Kante verbunden werden sowie Schwellenwerte zum Filtern selten vorkommender Sequenzen ab einer bestimmten Clustergröße, die als Hintergrundrauschen nicht in der Berechnung Berücksichtigung finden sollen. \\

Zur Konstruktion des Graphen wird die Python-Library graph-tool ~\cite{peixoto_2014} genutzt. Die Knoten werden aus den FASTQ-Daten der getrimmten Reads mittels SeqIO aus der Library Biopython ~\cite{cock_2009_1} ausgelesen und im Graphen mit den Knoteneigenschaften ihrer Basensequenz, einer internen ID sowie Angaben zur Basenqualität abgelegt. Die Codierung des Qualitystrings der Reads variiert je nach verwendeter Platform. Daher wird er durch SeqIO ausgelesen und für jede Base in ein einheitliches Maß, den Phred Quality Score $ Q $, decodiert ~\cite{cock_2009_2}. Zusätzlich wird aus den Phred Quality Scores die geschätzte Fehlerwahrscheinlichkeit $ P $ für jede Base nach Formel \eqref{eqn:3-1} bestimmt ~\cite{ewing_1998}.  

\begin{equation} \label{eqn:3-1}
    \tag{3-1}
    P = 10^{\frac{-Q}{10}}
\end{equation}

Für jeden Knoten werden die Vektoren mit den Phred Qualitiy Scores und den geschätzen Fehlerwahrscheinlichkeiten der Basen des Reads als Knoteneigenschaften gespeichert. \\

Die Laufzeit für das Hinzufügen eines Knotens beträgt nach der Dokumentation von graph-tool  $ O(V) $, da es sich hierbei um eine Einfügeoperation in die bereits bestehende Knotenmenge handelt und ein neuer Iterator über alle Knoten erzeugt und zurückgegeben wird ~\cite{docs_graph_tool}. Die Zuweisung der Knoteneigenschaften erfolgt in $ O(1) $. Über alle Reads, also über die resultierende Anzahl der Knoten $ V $ ergibt sich daraus eine Gesamtlaufzeit von $ O(V^2) $.\\

\subsection{Kanten des Graphen} \label{subsec:edges}
Die Kanten des Graphen definieren sich durch das mittels Minimap2 erzeugten Sequenzalignments (vgl. Kap. \ref{sec:edit}). Jedes Alignment zwischen zwei Reads entspricht im Graphen einer gerichteten Kante $e = (source,\; target)$, die den Vergleich der Query- zur Referenzsequenz repräsentiert. Sie verbindet somit zwei der zuvor aus der FASTQ-Daten erzeugten Knoten. Das Auslesen des sam-Formats des Alignmentfiles erfolgt mit Hilfe der Python-Library pysam ~\cite{pysam}. Dabei wird die Edit-Distanz aus dem NM-Tag zunächst genutzt, um nur Kanten in den Graphen aufzunehmen, die bereits einen optimierten Minimap2-Path darstellen. Liegen diese unterhalb des durch die Konfigurationsdatei festgelegten Grenzwertes, so wird die Kante dem Graphen hinzugefügt. Dabei werden als Kanteneigenschaften die Edit-Distanz, die CIGAR-Tupel sowie die aus der Basenqualität und Mutationsrate bestimmte Likelihood hinzugefügt. Zusätzlich kann zur Kontrolle oder für eine spätere Verwendung auch der CIGAR-String selbst als Kanteneigenschaft gespeichert werden, falls bei Minimap2 die Option zur Erzeugung des cs-Tags aktiviert wurde. Die CIGAR-Tupel werden durch pysam aus dem CIGAR-String geparsed, hierbei handelt es sich um eine Liste von Tupeln, die jeweils aus Integer-Wertepaaren bestehen. Der erste Wert jedes Tupels gibt die spezifische Operation des Matches oder Mismatches. So entspricht beispielsweise ein Wert von $ 7 $ oder $ 0 $ einem Match und ein Wert von $ 2 $ einer Deletion. Der zweite Werte jedes Tupels gibt die Anzahl der Basen an, die von der entsprechenden Operation betroffen sind. \\

Diese CIGAR-Tupel werden für die Berechnung der Likelihood zwischen zwei Knoten benötigt, dies erfolgt in der Methode \lstinline|get_alignment_likelihood()| (Algorithmus \ref{alg:lh_read}) aus dem Modul \lstinline|likelihood_operations|. Dabei wird aus den p-Werten der Basenqualität für jede Base der Query-Sequenz die Wahrscheinlichkeit errechnet, dass es sich im Falle eines Matches um die korrekte Base handelt  \eqref{eqn:3-2} bzw. im Falle eines Mismatches, dass es sich um einen Sequenzierfehler \eqref{eqn:3-3} oder um eine Mutation handelt \eqref{eqn:3-4}. \\

Die Berechnung der Likelihoods für die paarweisen Vergleiche der Reads basiert auf dem in Kap. ~\ref{subsec:phmm} beschriebenen pair Hidden Markov Model. Hierbei repräsentiert das durch Minimap2 bestimmte Sequenzalignment bereits den wahrscheinlichsten Pfad durch die pairHMM-Matrix. Da dieser Pfad ohnehin die Wahrscheinlichkeit des pairHMM dominieren würde, wird zugunsten der Laufzeit direkt auf das Alignment von Minimap2 zurückgegriffen, um die Likelihoods zwischen den Readsequenzen zu bestimmen. Dabei werden die Sequenzierfehlerrate $ \epsilon $ und Basenqualität $ q_{query} $ durch die bereits zuvor ermittelte geschätzte Fehlerrate $ p_{query} $ berücksichtigt. Die Likelihood $ pairHMM_{\epsilon, q_{query}} \;(s_{ref}\;|\; s_{query}) $, dass der Queryread aus dem Referenzread allein durch Sequenzierfehler und Mutationen entstanden ist, errechnet sich schließlich aus dem Produkt der Likelihoods $ L_{i} $ für jede Base $ b $ an jeder Position $ i $ innerhalb der Sequenz $ s $ des Queryreads $ s_{query} $ im Vergleich zum Referenzread $ s_{ref} $.
\begin{equation} \label{eqn:3-2}
\tag{3-2}
pairHMM_{\epsilon, q_{query}} \;(s_{query}\;|\; s_{ref}) = \prod_{i=1}^{k}L_{i}
\end{equation}

Jede Base $ b $ an Position $ i $ einer Readsequenz $ s $ der Länge $ k $ lässt sich also definieren als $ b \in \{\,b_{i}\in \{A,C,G,T\}^k\;,\; b_{i} \in s \;|\; i = 1, \dotsb, k \,\}$. Seien $ b_{i\,_{ref}} $ und $ b_{i\,_{query}} $ die Basen der Query- und der Referenzsequenzen an Position $ i $ einer Sequenz und $  p_{i\,_{query}} $ die geschätzte Fehlerrate von $ b_{i\,_{query}} $, die sich aus dem Phred Quality Score $ Q $ nach  \eqref{eqn:3-1} ergibt. Seien zudem $ m_{sub} $, $ m_{ins} $ und $ m_{del} $ die über die Konfigurationsdatei festgelegten Mutationsraten für Substitutionen, Insertionen und Deletionen. Dann errechnet sich die Likelihood $ L_{i} = Pr(b_{i\,_{ref}}\;|\; b_{i\,_{query}})$ an der Position $ i $ im Falle eine Matches unter Berücksichtigung der geschätzten Fehlerrate durch:
\begin{equation} \label{eqn:3-3}
\tag{3-3}
L_{i\,_{match}} = 1 - p_{i\,_{query}}
\end{equation}

Bei einem Mismatch dagegen müssen die Wahrscheinlichkeiten von Mutationen und Sequenzierfehlern berücksichtigt werden. Im Falle einer Mutation muss in die Wahrscheinlichkeit eines Matches auch die Mutationsrate des aufgetretenen Mismatches $ m_{rate} \in \{m_{sub} $, $ m_{ins} $, $ m_{del}\} $ einbezogen werden:
\begin{equation} \label{eqn:3-4}
\tag{3-4}
L_{i\,_{mut}} = m_{rate}\; \cdotp \;(1 - p_{i\,_{query}})
\end{equation}

Die Wahrscheinlichkeit eines Sequenzierfehlers, also dass anstelle der sequenzierten Base tatsächlich eine der drei anderen Basen vorliegt, entspricht $ 1/3 $ der geschätzten Fehlerrate des Phred Quality Scores:
\begin{equation} \label{eqn:3-5}
\tag{3-5}
L_{i\,_{seqerr}} = \frac{1}{3} \; \cdotp \; p_{i\,_{query}}
\end{equation}

Aus \eqref{eqn:3-4} und \eqref{eqn:3-5} errechnet sich also die Likelihood bei einem Mismatch durch:
\begin{equation} \label{eqn:3-6}
\tag{3-6}
L_{i\,_{mismatch}} = (1-m_{rate}) \; \cdotp \; L_{seqerr} \; \cdotp \; L_{mut}
\end{equation}

Aus den Liklihoods von Matches \eqref{eqn:3-3} und Mismatches \eqref{eqn:3-4} kann somit schließlich nach \eqref{eqn:3-2} die Likelihood zwischen den Reads paarweise bestimmt werden.\\

Für eine existierende Kante, von der die CIGAR-Tupel bekannt sind, kann die Methode \lstinline|get_alignment_likelihood()| zudem die Likelihood in entgegengesetzter Richtung bestimmen. Dabei wird der Queryread als Referenzread betrachtet und umgekehrt. Dies ist über das boolsche Argument \lstinline{reverse} steuerbar. Gilt \lstinline|reverse = True|, so werden für die übergebenen CIGAR-Tupel Insertionen zu Deletionen und Deletionen zu Insertionen umbewandelt, anschließend wird die Likelihood nach \eqref{eqn:3-6} berechnet.

Zur zusätzlichen Veranschaulichung ist die Methode \lstinline|get_alignment_likelihood()| in Algorithmus \ref{alg:lh_read} in Pseudocode dargestellt.

\begin{algorithm}[H]
	\caption{Berechnung der Likelihood zwischen zwei Reads}  \label{alg:lh_read}
	\begin{algorithmic}[1]	
		\Function{get\_alignment\_likelihood}{$ m_{sub} $, $ m_{ins} $, $ m_{del} $, $ CIGAR-Tuples $, $ p_{query} $, reverse}
		\State $ likelihood \gets 1.0 $, $ index \gets 0 $
		\If {$reverse$}
		    \State swap values of $ m_{ins} $ and $ m_{del} $
	    \EndIf
	    \ForAll {$ (operation, length) \in CIGAR-Tuples $}
	    \If {$operation \in match $}
		    \While{$ index < length $}
		        \State $ likelihood\, \gets likelihood \,\cdotp (1-p_{query}[index]) $
		    	\State $ index \gets index + 1 $
		    \EndWhile
	    \EndIf
	    \If {$operation \in mismatch $}
	        \State $ m_{rate} \gets 0 $
	        \If {$operation \in substitution $}
	            \State $ m_{rate} \gets m_{sub} $
	        \EndIf
	        \If {$operation \in insertion $}
	            \State $ m_{rate} \gets m_{ins} $
	        \EndIf
	        \If {$operation \in deletion $}
	            \State $ m_{rate} \gets m_{del} $
	        \EndIf
	        \While{$ index < length $}
		        \State $ likelihood\, \gets likelihood \,\cdotp (1 - m_{rate})\,\cdotp \frac{1}{3} \,\cdotp p_{query}[index] \, +  m_{rate}\,\cdotp $         
		         \State \hspace{63pt}  $ (1 - p_{query}[index]) $ 		        
		        \State $ index \gets index + 1 $
	        \EndWhile
	    \EndIf
		\EndFor
		\State \Return $likelihood$
		\EndFunction		
	\end{algorithmic}
\end{algorithm}

Hinsichtlich der Laufzeit benötigt das Hinzufügen einer Kante nach Angaben der graph-tool Dokumentation ~\cite{docs_graph_tool} eine Laufzeit von $ O(1) $. Da aber die Query- und die Referenzreads den bereits zuvor angelegten Knoten zugeordnet werden müssen, erfordert dies eine Suche der betreffenden Knoten. Dabei durchsucht graph-tool mit seiner Funktion \lstinline|find_vertex()| allein die Knoten und prüft auf die gesuchte Read-ID aus den FASTQ-Daten. Die ein- und ausgehenden Kanten der Knoten werden nicht beachtet, so dass eine Tiefen- oder Breitensuche des Graphen nicht notwendig ist und die Suche in $ O(V) $ durchgeführt werden kann ~\cite{graph_tool_coplexity_find_vertex}. Die Zuweisung der Kanteneigenschaften erfolgt jeweils in $ O(1) $, da diese direkt bei der Erzeugung der Kante hinzugefügt werden und keine vorherige Suche der Kante erforderlich ist. Für die Berechnung der Likelihood wird die geschätzte Fehlerrate $ p_{query} $ jeder Base verwendet, so dass die Anzahl der Berechnungen für jede Kante der Länge der Readsequenz $ k $ entspricht. Die Laufzeit für das Hinzufügen einer Kante beträgt somit $ O(k) $. Für alle Kanten ergibt sich daraus eine Gesamtlaufzeit von $ O(E\, \cdotp (k + V)) $. Bei realen Datensätzen gilt in der Regel $ k << V $ und die Länge der Reads variiert nur in einem engen Bereich, so dass $ k $ als vernachlässigbar klein und als nahezu konstant betrachtet werden kann. Die Gesamtlaufzeit kann dort also auf $ O(E\, \cdotp V) $ geschätzt werden. \\

Nach Abschluss der Graphkonstruktion werden für jedes Individuum noch einige Statistiken in die Log-Dateien geschrieben. Hier werden neben der Anzahl der Knoten und Kanten des Graphen auch die Anzahl der Substitutionen bzw. SNPs, Insertionen und Deletionen festgehalten, die beim Auslesen der CIGAR-Tupel registriert wurden. Zudem findet sich hier auch die maximal vorkommenden Edit-Distanz über alle Knoten, sofern sich diese unterhalb des festgelegten Schwellenwertes liegt. Ansonsten entspricht sie dem in der Konfigurationsdatei angegebenen Schwellenwert.\\

Als optionaler Output können über die Konfigurationsdatei und die Snakemake-Regel \lstinline|rule noderad| auch die detaillierten Graphinformationen sowie eine Visualisierung des Graphen ausgegeben werden. Die Graphinformationen wie Knoten, Kanten und ihre Eigenschaften können dabei im GraphMl-, DOT-, GML- oder im binären gt-Format gespeichert werden. Die graphische Darstellung wird als pdf-Datei ausgegeben, dabei entspricht die Kantenfärbung der berechneten Likelihood zwischen den Reads.. \\

\subsection{Bestimmung der Zusammenhangskomponenten} \label{subsec:comp}

Die Bestimmung und Indexierung der Zusammenhangskomponenten erfolgt durch graph-tool selbst und kann in $ O(V + E) $ durchgeführt werden ~\cite{docs_graph_tool}. Die Indexnummer jeder Zusammenhangskomponente wird den in ihr enthaltenen Knoten als Knoteneigenschaft hinzugefügt. Zusammenhangskomponenten mit mehr als einem Knoten  werden als neuer eigenständiger Graph initialisiert und in einer Liste abgelegt. Hierfür wird aus dem Graphen für jede Komponente eine gefilterte Sicht erzeugt, die als neues Graph-Object gespeichert wird. Der Filtervorgang jeder Zusammenhangskomponente $ C $ muss für alle Knoten des Graphen durchgeführt werden, daher beträgt die Laufzeit hierfür $ O(C \, \cdotp V) $. Da alle weiteren Schritte des Algorithmus jeweils auf den einzelnen Komponenten durchgeführt werden, kann durch die Verwendung einer Liste von Graphen im Folgenden eine einfachen Iteration über die Komponenten in $ O(C) $ ausgeführt werden, ohne dass der Filtervorgang über alle Knoten jeder Komponente wiederholt werden muss. Zudem ermöglicht diese Datenstruktur eine effizientere Traversierung und Suche innerhalb der Zusammenhangskomponente, ohne dass für jede Komponente der gesamte Graph betrachtet werden muss. Der ursprüngliche Graph wird anschließend entfernt, um Arbeitsspeicher freizugeben. Auch dies erfolgt in konstanter Zeit. \\

In der Log-Datei wird die Anzahl der Knoten aller Zusammenhangskomponenten als Histogramm festgehalten. Ebenso wird dort für alle Komponenten mit mehr als einem Element die Anzahl ihrer Knoten, Kanten und Eigenschaften aufgelistet.

Über die Konfigurationsdatei und die Snakemake-Regel \lstinline|rule noderad| können optional auch für die Zusammenhangskomponenten jeweils Visualisierungen und detaillierte Graphinformationen in den oben genannten Formaten (Kap. \ref{subsec:edges}) ausgegeben werden. Zudem kann optional auch der gesamte Graph mit den Komponentenidizes als Knoteneigenschaften gespeichert werden. In der visuellen Darstellung werden seine Knoten entsprechend der zugehörigen Zusammenhangskomponente eingefärbt, seine Kantenfärbung richtet sich weiterhin nach der aus \eqref{eqn:3-3} resultierenden Likelihood.

\subsection{Laufzeitanalyse zur Konstruktion des Graphen}
Wie an entsprechender Stelle bereits beschrieben, ist für die Erzeugung der Knoten eine Laufzeit von $ O(V^2) $ (Kap. \ref{subsec:nodes}) erforderlich, das Hinzufügen der Kanten kann in $ O(E\, \cdotp V) $ erfolgen (Kap. \ref{subsec:edges}). Somit wird für den vollständigen Aufbau des Graphen eine Laufzeit von $ O(V \, \cdotp (V+E)) $ benötigt. In Zusammenschau mit der für die Extraktion der Zusammenhangskomponenten erforderliche Laufzeit von $ O(C \, \cdotp V + (V + E)) $ (Kap. \ref{subsec:comp}) ergibt sich daraus nach  \eqref{eqn:3-7} eine Laufzeit von $ O(V \, \cdotp (V + C) + E \, \cdotp (V + 1)) $. 
\begin{equation} \label{eqn:3-7}
\tag{3-7}
O(V \, \cdotp (V + E + C) + E) = O(V \, \cdotp (V + C) + E \, \cdotp (V + 1))
\end{equation}

Bei realen Daten gibt es in der Regel deutlich mehr Knoten als Cluster bzw. Zusammenhangskomponenten, so dass gilt $ C < V $. Würde im Worst Case jede Zusammenhangskomponente aus nur einem Knoten bestehen, also $ C = V $, so gilt $ O(V + C) = O(V + V) = O(V) $. Ebenso gilt $ O(V + 1) = O(V) $. Unter der Voraussetzung aus Kap. \ref{subsec:nodes}, dass für die Readlänge $ k $ gilt, dass $ k << V $, folgt nach  \eqref{eqn:3-8} eine Gesamtlaufzeit von $ O(V \, \cdotp (V + E)) $.
\begin{equation} \label{eqn:3-8}
\tag{3-8}
O(V \, \cdotp (V + C) + E \, \cdotp (V + 1)) = O (V^2 + E \, \cdotp V) = O(V \, \cdotp (V + E))
\end{equation}

Sind bei kleinen Datensätzen nur wenige Reads vorhanden, so dass gilt $ k \leq V $, dann ergibt sich unter zusätzlicher Berücksichtigung von $ k $ im Worst Case mit $ k = V $ und mit $ C = V $ nach \eqref{eqn:3-9} ebenfalls eine Gesamtlaufzeit von $ O(V \, \cdotp (V + E)) $.
\begin{equation} \label{eqn:3-9}
\tag{3-9}
\begin{aligned}
&\ {} O(V^2) + O((V + k) \, \cdotp E) + O(V + E) + O(C \, \cdotp V) \\
& \ = O(V^2 + E \, \cdotp V + k \, \cdotp E) + O(V + E + C \, \cdotp V)\\
&\ = O(V^2 + C \, \cdotp V + V + E + E \, \cdotp V + k \, \cdotp E) \\
&\ = O(V \, \cdotp (V + C + 1) + E \, \cdotp (1 + V + k))\\
&\ = O(V \, \cdotp (V + C) + E \, \cdotp (V + k))\\
&\ = O(V \, \cdotp (V + V) + E \, \cdotp (V + V))\\
&\ = O( 2 \, \cdotp V \, \cdotp (V + E))\\
&\ = O(V \, \cdotp (V + E))\\
\end{aligned}
\end{equation}

Unter der Annahme, dass in seltenen Fällen die Readlänge, die meist nur wenige hundert Basenpaare zählt, tatsächlich die Anzahl der Reads übersteigt und somit $ k > V $, kann die Gesamtlaufzeit unter direkter Berücksichtigung von $ k $ nach \eqref{eqn:3-10} mit $ O(V^2 + E \, \cdotp (V + k)) $ angegeben werden. Da der Datensatz in diesem Fall relativ klein ist, hätte die damit verbundene höhere Laufzeit dennoch nur geringe Auswirkungen.
\begin{equation} \label{eqn:3-10}
\tag{3-10}
\begin{aligned}
&\ {} O(V^2) + O((V + k) \, \cdotp E) + O(V + E) + O(C \, \cdotp V) \\
&\ = O(V \, \cdotp (V + C) + E \, \cdotp (V + k))\\
&\ = O(V^2 + E \, \cdotp (V + k))\\
\end{aligned}
\end{equation}

\section{Bestimmung der maximalen Likelihood der Allele} \label{sec:lh_allele}
\subsection{} \label{subsec:}
======================= draft =======================\\
Die einzelnen Zusammenhangskomponenten repräsentieren einen oder mehrere Loci.\\
0. get\_candidate\_alleles(): gibt die in der comp vorkommmenden seq lexikographisch sortiert fpr deterministischere Ergebnisse zurück. Übersteigt die Größe des Clusters, d.h. die Knotenzahl einer Zusammenhangskomponenten einen in der Konfigurationsdatei als treshold-cluster-size festgelegten Wert, so wird von den im Cluster vorkommenden Sequenzen zunächst die absolute Häufigkeit bestimmt. Es werden dann nur diejenigen sequenzen lexikographisch sortier zurückgegeben, die einen weiteren, ebenfalls in der config festgelegten schwellenwert, 'treshold-seq-noise', überschreiten.\\
-> filtern: noise-seq werden nicht als allel-kandidaten ausgewählt \\
vafs kandidaten bestimmen:\\
1. tatsächlich zu erwartende Anzahl von allelen anhand der ploidie bestimmen get\_max\_parsimony\_n\_alleles:
\begin{itemize}
	\item aus anzahl der beobachteten allele (länge der allel-liste, $n_{observed}$) und ploidie
	\item get\_max\_parsimony\_n\_alleles(): bestimmung der erwarteten tatsächlichen anzahl $n_{alleles}$von allelen aus der Anzahl der beobachteten allele $n_{observed}$ in abhänigkeit von der ploidie
	\item durch die ploidie werden unnötige und unmögliche allel-kombinationen   eingespart und in der weiteren berechnung nicht berücksichtigt
	\item Fall ploidie ist höher als die anzahl der beobachteten allele, so muss es mindestens so viele ggf. identische allele geben (-> Homozygotie), damit die ploidie erfüllt werden kann\\
	\item wurden dagegen mehr allele beobachtet als aufgrund der ploidie möglich sind und die ploidie ist teiler von $n_{observed}$, so wurden genauso viele allele beobachtet, wie auch möglich sind und es gilt $n_{observed} = n_{alleles}$. 
	\item gilt $n_{observed}>ploidie$ und ist die beobachtete anzahl von allelen aber nicht ganzzahlig durch die ploidie teilbar, so $n_{alleles}$ so erhöht werden, dass die ploidie ein teiler hiervon ist, d.h .es müssen tatsächlich so viele allele vorkommen, dass eine korrekte ploidie erreicht wird. es muss also um die ploidie abzüglich des restes aus der restdivision erhöht werden: $ n_{alleles} = n_{observed} + ploidy - (n_{observed} \mod ploidy)$
\end{itemize}

\noindent 2.alle kombinationen möglicher häufigkeitsverteilungen zur ermittelten anzahl von allelen $ n_{alleles} $ bestimmen get\_candidate\_vafs:\\
\begin{itemize}
	\item Allel-Verteilung durch Urnenmodell bestimmen: k elem werden ausgewählt, reihenfolge nicht relevant -> binominalkoeffizient $\binom{n}{k} = \frac{n!}{(n-k)!\, \cdotp k!}$ \\
	\item alle Kombinationen mit wiederholung für $n_{alleles} $ bestimmen -> mehrfaches vorkommen erlaubt -> n in binomialkoeffizient wird durch $(n + k -1)$ ersetzt: $\binom{n + k - 1}{k} = \frac{(n+k-1)!}{(n-1)!\, \cdotp k!}$\\ 
	durch combinations\_with\_replacement  aus python-library itertools umgesetzt:
	Bsp.: \\	
	$ n_{alleles} = 2$: \\
	{[(0, 0), (0, 1), (1, 1)]}\\
	$ n_{alleles} = 3$: \\
	{[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 2, 2), (0, 1, 1, 1), (0, 1, 1, 2), (0, 1, 2, 2), (0, 2, 2, 2), (1, 1, 1, 1), (1, 1, 1, 2), (1, 1, 2, 2), (1, 2, 2, 2), (2, 2, 2, 2)]}\\
		
	\item relative Häufigkeiten $ h_{n} $  aus den absoluten Häufigkeiten jedes allesl aus jeder Kombination der allelverteilungen bestimmen:  $ h_{n} = \frac{H_{n}} {n_{alleles}} $\\
	Bsp.: \\
	$ n_{alleles} = 2$: \\
	{[1.0, 0.0]}, {[0.5, 0.5]}, {[0.0, 1.0]} \\
	$ n_{alleles} = 3$: \\
	{[{[1.0, 0.0, 0.0]}, {[0.75, 0.25, 0.0]}, {[0.75, 0.0, 0.25]}, {[0.5, 0.5, 0.0]}, {[0.5, 0.25, 0.25]}, {[0.5, 0.0, 0.5]},
		{[0.25, 0.75, 0.0]}, {[0.25, 0.5, 0.25]}, {[0.25, 0.25, 0.5]}, {[0.25, 0.0, 0.75]}, {[0.0, 1.0, 0.0]}, {[0.0, 0.75, 0.25]}, {[0.0, 0.5, 0.5]}, {[0.0, 0.25, 0.75]}, {[0.0, 0.0, 1.0]}]} \\
	\item die relativen Häufigkeiten der Allelverteilungen sollen im weiteren als VAF (Variant allele frequency) oder Fraktionen bezeichnet werden. Sie entsprechen den in Kap. \ref{subsec:sol_allele_lh} beschriebenen Franktionen $ \Theta = (\theta_{1}, \dots , \theta_{n}) $\\
\end{itemize}

2. Berechnung der Likelihoods anhand der möglichen Häufigkeitsverteilungen:\\
2.1 Berechnung der Wslk, einen Read zu beobachten anhand der gegebenen Allele-Fraktion, get\_allele\_likelihood\_read(): Vgl Kap. \ref{subsec:sol_allele_lh}, Formel ....(step 2.1)
\begin{itemize}
	\item für jeden Read soll zunächst die Wslk gesucht werden, dass der Read durch seq-fehler aus einem der Kanditatenallele entstanden ist. 
	\item hierzu werden für jeden Read $ r_{i} $ die ausgehende Kanten gesucht, die jeweils zu jedem der Kandidatenallele $ a_{i} $ führen und die nach \eqref{eqn:3-2} berechnete Likelihood zurückgegeben.
	\item hierbei werden, geordnet nach Laufzeitaufwand verschiedene möglichkeiten geprüft: 
	\item Fall (1) es existiert eine solche Kanten, dann kann die likelihood direkt zurückgegeben werden; 
	\item Fall (2) eine solche Kante existiert nicht, aber es gibt eine entgegengesetzt gerichtete Kanten, dann kann die gesuchte Likelihood des Gegenrichtung, also mit reverese = True über die funktion get\_alignment\_likelihood() bestimmt werden; 
	\item Fall (3) im gesamten Graphen wird nach einer Kante gesucht, die zwischen Knoten verläuft, deren seqenzen denen von $ r_{i} $ und $ a_{i} $ entsprechen. Existiert eine solche Kanten so werden die CIGAR-Tupel dieser Kante verwendet, um für $ r_{i} $ die likelihood zu bestimmen. Da die CIGAR-Tupel allein über die Sequenz definiert werden, gilt das so ermittelte CIGAR-Tupel wegen identischer Sequenzen der begrenzenden Knoten auch für eine nicht existierenden Kante zwischen $ r_{i} $ und $ a_{i} $. Für die Berechnung der Likelihood können diese CIGAR-Tupel also zusammen den p-Werten des Phred Quality Scores (aus \eqref{eqn:3-1}) von $ r_{i} $ die Likelihood mit Hilfe der Funktion get\_alignment\_likelihood() berechnet. 
	\item Fall (4) existiert im gesamten Graphen keine vergleichbare Kante, die $ r_{i} $ und $ a_{i} $ mit einander verbindet, so wird der Wert Null zurückgegeben.
	\item laufzeit:\\	 
	 Laufzeit Kante suchen bei bekannter source und target Knoten $O(\min (d(r_{i}), d(a_{i})))$, mit den Knotengraden $d(r_{i}) $von Knoten $r_{i}$ und $d(a_{i}) $ von Knoten $a_{i}$ ~\cite{docs_graph_tool} -> Worst Case sind alle Knoten des Graphen ausschließlich mit den beiden Knoten $r_{i}$ und $a_{i}$ verbunden, also haben $r_{i}$ und $a_{i}$ jeweils einen Knotengrad, welcher der Anzahl der Knoten des Graphen entspricht mit Ausnahme des Knotens selbst, also : $O(V+\min (d(r_{i}), d(a_{i}))) = O(V+\min (d(V+1), d(V-1))) = O(V+V-1) = O(2 \, \cdotp V) = O(V)$ \\
	 Worst-Case muss die Laufzeit aller Fälle summiert werden:\\
	 Fall (1): über alle out\_neighbors $d(r_{i_{out}})$ des Knotens $r_{i}$ Kante suchen im Worst Case hat $r_{i}$ nur ausgehende Kanten zu allen Knoten des Graphen -> in $ O(d(r_{i_{out}}) \, \cdotp V) = O((V-1) \, \cdotp V) = O(V^2)$, außerdem die Likelihood als Kanteneigenschaft abrufen in $ O(1) $ => also insgesamt $ O(V^2) + O(1)=O(V^2)$\\
	 Fall (2): analog über alle in\_neighbors $d(r_{i_{in}})$ ebenfalls in $O(V^2)$ aber mit Likelihoodberechnung der rückläufigen Kanten in $O(E \, \cdotp (V+k))$ => insgesamt $ O(V^2 + E \, \cdotp (V + k)) $ \\
	 Fall (3): cigar-Tupel finden $O(V)$ (siehe laufzeitberechnung cigar tupel) + likelihoodberechnung $O(E \, \cdotp (V+k))$ => gesamt: $ O(V + E \, \cdotp (V + k)) $\\
	 Fall (4): konstanten Wert zurückgeben in $ O(1) $\\
	 => Gesamt Fall 1 bis 4: \\
	 \begin{equation} \label{eqn:3-xxx}
	 \tag{3-xxx}
	 \begin{aligned}
	 &\ {} O(V^2) + O(V^2 + E \, \cdotp (V + k)) + O(V + E \, \cdotp (V + k)) + O(1)  \\
	 & \ = O(2 \, \cdotp V^2 +2 \, \cdotp E \, \cdotp (V + k) + V + E\\
	 &\ = O(V^2 + E \, \cdotp (V + k) + V + E) \\
	 &\ = O(V \, \cdotp (V + 1) + E \, \cdotp (1 + V + k))\\
	 &\ = O(V  \, \cdotp  V + E \, \cdotp  (V + k))\\
	 &\ = O(V^2 + E \, \cdotp (V + k))\\
	 \end{aligned}
	 \end{equation}
	 entspricht also der Laufzeit zur Konstruktion des Graphen im Worst Case
	\item pseudocode
	%\begin{algorithm}[H]
	%	\caption{Likelihood zwischen Read und Allel bestimmen}  \label{alg:cig}
	%	\begin{algorithmic}[1]	
	%ToDo -> to pseudocode
	%def get_allele_likelihood_read(comp, allele, node):
	   % # obtian one arbitrary out edge of node that points to another node with 
	   % # sequence = allele
       % out_neighbors = comp.get_out_neighbors(node)
       % in_neighbors = comp.get_in_neighbors(node)
       % # search for existing edge from given node to target node with 
       % # sequence = allele
       % for neighbor in out_neighbors:
       %     if comp.vertex_properties['sequence'][neighbor] == allele:
       %         return comp.edge_properties["likelihood"][comp.edge(node, neighbor)]
       % # search for reverse target node with sequence = allele to given node
       % qual = comp.vp["quality"][node]
       % for neighbor in in_neighbors:
       %     if comp.vertex_properties['sequence'][neighbor] == allele:
       %         return get_alignment_likelihood(comp, 
       %         list(eval(comp.edge_properties['cigar-tuples'][comp.edge(neighbor,  
       %         node)])), qual, reverse=True)
       % cigar_tuples = get_cigar_tuples(comp, comp.vp["sequence"][node], allele)
       % if cigar_tuples:
       %     return get_alignment_likelihood(comp, list(eval(cigar_tuples[0])), qual, 
       %     reverse=cigar_tuples[1])
       % return 0
	%	\end{algorithmic}
	%\end{algorithm}
\end{itemize}
Methode get\_cigar\_tuples():\\
\begin{itemize}
	\item argument sind 2 sequenzen $ s_{source} $ und $ s_{target} $ -> rückgabe CIGAR-Tupel einer kante zwischen source und target und eines boolschen Wertes für das argument 'reverse' der Funktion get\_alignment\_likelihood(), die im anschluss die übergebenenen CIGAR-Tupel zur likelihood berechnung verwendet. Verläuft die Kante von $ s_{source} $ zu $ s_{target} $ so ist dieser Wert False, verläuft sie dagegen in Gegenrichtung so gilt reverse = True. \\
    \item Algorithmus:
	\item 1. im Graphen Menge aller Knoten suchen $ S_{source} $, die die sequenz $ s_{source} $ besitzen $O(V)$ (vgl. Kap. \ref{subsec:edges}) \\
	\item 2. im Graphen Menge aller Knoten suchen $ S_{target} $, die die sequenz $ s_{target} $ besitzen $ O(V) $\\
	\item 3. im Graphen eine Kante suchen die einen der Knoten $v$ aus $ S_{source} $ mit einem der Knoten $w$ aus $ S_{target} $ verbindet. Falls eine solche Kanten existiert, dann werden die CIGAR-Tupel dieser Kanten sowie der boolsche Wert False zurückgegeben. $O(\min (d(v), d(w)))$, mit den Knotengraden $d(v) $von Knoten $v$ und $d(w) $ von Knoten $w$ ~\cite{docs_graph_tool}\\
	\item 4. war die Suche aus 3. erfolglos, so wird eine Kante in Gegenrichtung gesucht, d.h. eine Kante die zwischen einem Knoten aus $ S_{target} $ zu einem Knoten aus $ S_{source} $ verläuft. Wurde eine solche Kanten gefunden, so werden ihre CIGAR-Tupel zusammen mit dem boolschen Wert True zurückgegeben $O(E)$.\\
	\item 5. Existiert im gesamten Graphen keine Kante, die  $ s_{source} $ und $ s_{target} $ verbindet, so wird None zurückgegeben.$O(\min (d(v), d(w)))$\\
	\item -> gesamt $ O(V+\min (d(v), d(w))$	\\
	\item -> pseudocode\\

	%\begin{algorithm}[H]
	%	\caption{CIGAR-Tupel bestimmen}  \label{alg:cig}
	%	\begin{algorithmic}[1]	
		%ToDo -> to pseudocode
		%def get_cigar_tuples(comp, seq, allele):
		%    source_nodes = [comp.vertex_index[node] for node in
		%    find_vertex(comp, comp.vp["sequence"], seq)]
		%    target_nodes = [comp.vertex_index[node] for node in find_vertex(comp, comp.vp["sequence"], allele)]
		%    for node_s in source_nodes:
		%        for node_t in target_nodes:
		%            edge = comp.edge(node_s, node_t)
		%            if edge:
		%                return comp.edge_properties["cigar-tuples"][edge], False
		%            rev_edge = comp.edge(node_t, node_s)
		%            if rev_edge:
		%                return comp.edge_properties["cigar-tuples"][rev_edge], True
		%    return None
	%	\end{algorithmic}
	%\end{algorithm}
\end{itemize}
=> gesamtlaufzeit step 2.1: $O(V+\min (d(v), d(w)))$ für CIGAR-Tupel-Suche -> im Worst Case $O(V+\min (d(v), d(w))) = O(V)$ (Vgl. Laufzeit get\_allele\_likelihood\_read(), Fall 1), Likelihood read-allel $O(V^2 + E \, \cdotp (V + k))$

\section{Bestimmung der maximalen Likelihood der Loci} \label{sec:lh_loci}
\subsection{} \label{subsec:}


\section{Ausgabe der Loci im VCF-Format} \label{sec:vcf}
\subsection{} \label{subsec:}
